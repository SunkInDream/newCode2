import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler

class GCNLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features, bias=False)
        
    def forward(self, x, adj):
        # x: (batch, nodes, features)
        # adj: (nodes, nodes)
        support = self.linear(x)
        output = torch.bmm(adj.unsqueeze(0).expand(x.size(0), -1, -1), support)
        return output

class GRINet(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, n_layers=2, n_nodes=None):
        super().__init__()
        self.n_nodes = n_nodes
        
        # GRU cell
        self.gru = nn.GRUCell(input_dim + hidden_dim, hidden_dim)
        
        # Graph convolution layers
        self.gcn_input = GCNLayer(input_dim, hidden_dim)
        self.gcn_hidden = GCNLayer(hidden_dim, hidden_dim)
        
        # Output layer
        self.output_layer = nn.Linear(hidden_dim, input_dim)
        
        self.hidden_dim = hidden_dim
        
    def forward(self, x, adj, mask=None):
        # x: (seq_len, batch, nodes, features)
        # adj: (nodes, nodes) 
        seq_len, batch_size, n_nodes, input_dim = x.shape
        
        h = torch.zeros(batch_size, n_nodes, self.hidden_dim, device=x.device)
        outputs = []
        
        for t in range(seq_len):
            x_t = x[t]  # (batch, nodes, features)
            
            # Graph convolution on input
            gcn_input = F.relu(self.gcn_input(x_t, adj))
            
            # Graph convolution on hidden state
            gcn_hidden = F.relu(self.gcn_hidden(h, adj))
            
            # Concatenate for GRU input
            gru_input = torch.cat([gcn_input, gcn_hidden], dim=-1)
            
            # Update hidden state node by node
            h_new = torch.zeros_like(h)
            for i in range(n_nodes):
                h_new[:, i, :] = self.gru(gru_input[:, i, :], h[:, i, :])
            h = h_new
            
            # Output
            output = self.output_layer(h)
            outputs.append(output)
            
        return torch.stack(outputs, dim=0)

def create_adjacency_matrix(data, threshold=0.1):
    """Create adjacency matrix based on correlation"""
    # Remove NaN for correlation calculation
    data_clean = np.nan_to_num(data)
    corr_matrix = np.corrcoef(data_clean.T)
    adj = (np.abs(corr_matrix) > threshold).astype(float)
    np.fill_diagonal(adj, 0)  # Remove self loops
    return adj

def grin_impute(data_matrix, window_size=20, hidden_dim=64, epochs=100, lr=0.001, input_dim=None):
    """
    GRINå¡«è¡¥å‡½æ•° - ä¿®å¤å¡«è¡¥é€»è¾‘
    """
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    import numpy as np
    import pandas as pd
    
    # è‡ªåŠ¨æ¨æ–­è¾“å…¥ç»´åº¦
    if input_dim is None:
        input_dim = data_matrix.shape[1]
    
    seq_len, n_features = data_matrix.shape
    
    print(f"ğŸ”§ GRINé…ç½®: seq_len={seq_len}, n_features={n_features}, input_dim={input_dim}")
    print(f"åŸå§‹ç¼ºå¤±å€¼æ•°é‡: {np.isnan(data_matrix).sum()}")
    
    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
    if seq_len < window_size:
        window_size = max(1, seq_len // 2)
    
    class GRINModel(nn.Module):
        def __init__(self, input_dim, hidden_dim):
            super().__init__()
            self.input_dim = input_dim
            self.hidden_dim = hidden_dim
            
            self.rnn = nn.GRU(input_dim, hidden_dim, batch_first=True)
            self.fc1 = nn.Linear(hidden_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, input_dim)
            self.dropout = nn.Dropout(0.1)
            
        def forward(self, x):
            rnn_out, _ = self.rnn(x)
            out = F.relu(self.fc1(rnn_out))
            out = self.dropout(out)
            output = self.fc2(out)
            return output
    
    # âœ… æ•°æ®é¢„å¤„ç† - æ›´å¥½çš„åˆå§‹å¡«è¡¥ç­–ç•¥
    data = data_matrix.copy()
    original_mask = ~np.isnan(data_matrix)  # Trueè¡¨ç¤ºè§‚æµ‹å€¼ï¼ŒFalseè¡¨ç¤ºç¼ºå¤±å€¼
    
    # ç”¨å¤šç§æ–¹æ³•è¿›è¡Œåˆå§‹å¡«è¡¥
    data_filled = pd.DataFrame(data)
    
    # 1. å…ˆç”¨å‰å‘åå‘å¡«å……
    data_filled = data_filled.fillna(method='ffill').fillna(method='bfill')
    
    # 2. å‰©ä½™çš„ç”¨åˆ—å‡å€¼å¡«å……
    for col in range(data_filled.shape[1]):
        if data_filled.iloc[:, col].isna().any():
            col_mean = data_filled.iloc[:, col].mean()
            if not np.isnan(col_mean):
                data_filled.iloc[:, col] = data_filled.iloc[:, col].fillna(col_mean)
            else:
                # å¦‚æœå‡å€¼ä¹Ÿæ˜¯nanï¼Œç”¨0å¡«å……
                data_filled.iloc[:, col] = data_filled.iloc[:, col].fillna(0)
    
    data = data_filled.values
    print(f"åˆå§‹å¡«è¡¥åç¼ºå¤±å€¼: {np.isnan(data).sum()}")
    
    # è½¬æ¢ä¸ºå¼ é‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # âœ… ä¿®å¤æ»‘åŠ¨çª—å£åˆ›å»º - ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•
    effective_window_size = min(window_size, seq_len)
    
    if seq_len <= effective_window_size:
        # å¦‚æœåºåˆ—é•¿åº¦å°äºç­‰äºçª—å£å¤§å°ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªåºåˆ—
        X_windows = [data]
        masks_windows = [original_mask]
        window_positions = [0]
    else:
        # åˆ›å»ºé‡å çª—å£
        X_windows = []
        masks_windows = []
        window_positions = []
        
        step_size = max(1, effective_window_size // 2)
        
        for i in range(0, seq_len - effective_window_size + 1, step_size):
            end_idx = i + effective_window_size
            X_windows.append(data[i:end_idx])
            masks_windows.append(original_mask[i:end_idx])
            window_positions.append(i)
        
        # ç¡®ä¿æœ€åä¸€ä¸ªä½ç½®ä¹Ÿè¢«è¦†ç›–
        if window_positions[-1] + effective_window_size < seq_len:
            X_windows.append(data[-effective_window_size:])
            masks_windows.append(original_mask[-effective_window_size:])
            window_positions.append(seq_len - effective_window_size)
    
    X = np.stack(X_windows)
    masks = np.stack(masks_windows)
    
    print(f"ğŸ”§ åˆ›å»ºäº† {X.shape[0]} ä¸ªçª—å£ï¼Œæ¯ä¸ªçª—å£å¤§å°: {X.shape[1]} x {X.shape[2]}")
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_tensor = torch.FloatTensor(X).to(device)
    masks_tensor = torch.FloatTensor(masks.astype(float)).to(device)
    
    # åˆ›å»ºæ¨¡å‹
    model = GRINModel(input_dim=input_dim, hidden_dim=hidden_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    print(f"ğŸ”„ å¼€å§‹GRINè®­ç»ƒ: {epochs} epochs")
    
    # è®­ç»ƒæ¨¡å‹
    model.train()
    best_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    for epoch in range(epochs):
        optimizer.zero_grad()
        
        try:
            outputs = model(X_tensor)
            
            # åªåœ¨è§‚æµ‹å€¼ä½ç½®è®¡ç®—loss
            loss = F.mse_loss(outputs * masks_tensor, X_tensor * masks_tensor)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # æ—©åœæœºåˆ¶
            if loss.item() < best_loss:
                best_loss = loss.item()
                patience_counter = 0
            else:
                patience_counter += 1
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}")
            
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break
                
        except Exception as e:
            print(f"âŒ è®­ç»ƒé”™è¯¯ Epoch {epoch}: {e}")
            break
    
    # âœ… é¢„æµ‹å¹¶å¡«è¡¥ç¼ºå¤±å€¼
    model.eval()
    with torch.no_grad():
        try:
            predictions = model(X_tensor).cpu().numpy()
            
            # âœ… æ”¹è¿›çš„ç»“æœé‡æ„é€»è¾‘
            result = data_matrix.copy()  # ä»åŸå§‹æ•°æ®å¼€å§‹
            
            print(f"ğŸ”§ å¼€å§‹å¡«è¡¥ç¼ºå¤±å€¼...")
            
            # ä¸ºæ¯ä¸ªä½ç½®æ”¶é›†æ‰€æœ‰å¯èƒ½çš„é¢„æµ‹å€¼
            prediction_counts = np.zeros_like(data_matrix)
            prediction_sums = np.zeros_like(data_matrix)
            
            # éå†æ‰€æœ‰çª—å£çš„é¢„æµ‹ç»“æœ
            for window_idx, start_pos in enumerate(window_positions):
                if window_idx >= predictions.shape[0]:
                    break
                
                pred_window = predictions[window_idx]  # (window_size, n_features)
                
                # å°†çª—å£é¢„æµ‹ç»“æœç´¯åŠ åˆ°å¯¹åº”ä½ç½®
                for t in range(effective_window_size):
                    actual_pos = start_pos + t
                    if actual_pos < seq_len:
                        # åªåœ¨åŸå§‹ç¼ºå¤±ä½ç½®ç´¯åŠ é¢„æµ‹å€¼
                        missing_mask = np.isnan(data_matrix[actual_pos, :])
                        
                        prediction_sums[actual_pos, missing_mask] += pred_window[t, missing_mask]
                        prediction_counts[actual_pos, missing_mask] += 1
            
            # è®¡ç®—å¹³å‡é¢„æµ‹å€¼å¹¶å¡«è¡¥
            filled_count = 0
            for i in range(seq_len):
                for j in range(n_features):
                    if np.isnan(data_matrix[i, j]) and prediction_counts[i, j] > 0:
                        result[i, j] = prediction_sums[i, j] / prediction_counts[i, j]
                        filled_count += 1
            
            print(f"âœ… é€šè¿‡æ¨¡å‹é¢„æµ‹å¡«è¡¥äº† {filled_count} ä¸ªç¼ºå¤±å€¼")
            
            # âœ… å¯¹äºä»ç„¶ç¼ºå¤±çš„å€¼ï¼Œç”¨å¤‡ç”¨ç­–ç•¥å¡«è¡¥
            remaining_missing = np.isnan(result)
            if remaining_missing.any():
                remaining_count = remaining_missing.sum()
                print(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨ç­–ç•¥å¡«è¡¥å‰©ä½™çš„ {remaining_count} ä¸ªç¼ºå¤±å€¼")
                
                # ç”¨åˆ—å‡å€¼å¡«è¡¥å‰©ä½™ç¼ºå¤±å€¼
                for j in range(n_features):
                    col_missing = remaining_missing[:, j]
                    if col_missing.any():
                        # è®¡ç®—è¯¥åˆ—è§‚æµ‹å€¼çš„å‡å€¼
                        observed_values = result[~np.isnan(result[:, j]), j]
                        if len(observed_values) > 0:
                            col_mean = np.mean(observed_values)
                            result[col_missing, j] = col_mean
                        else:
                            # å¦‚æœè¯¥åˆ—å®Œå…¨æ²¡æœ‰è§‚æµ‹å€¼ï¼Œç”¨0å¡«è¡¥
                            result[col_missing, j] = 0
            
            final_missing = np.isnan(result).sum()
            print(f"âœ… GRINå¡«è¡¥å®Œæˆ")
            print(f"å¡«è¡¥å‰ç¼ºå¤±å€¼: {np.isnan(data_matrix).sum()}")
            print(f"å¡«è¡¥åç¼ºå¤±å€¼: {final_missing}")
            
            return result
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹é˜¶æ®µé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            
            # å®Œå…¨å›é€€åˆ°ç®€å•å¡«è¡¥
            print("ğŸ”„ å›é€€åˆ°ç®€å•å‡å€¼å¡«è¡¥...")
            result = data_matrix.copy()
            
            for j in range(n_features):
                col_data = result[:, j]
                col_missing = np.isnan(col_data)
                
                if col_missing.any():
                    observed_values = col_data[~col_missing]
                    if len(observed_values) > 0:
                        col_mean = np.mean(observed_values)
                        result[col_missing, j] = col_mean
                    else:
                        result[col_missing, j] = 0
            
            return result

# âœ… æµ‹è¯•å‡½æ•°
def test_grin_with_different_dims():
    """æµ‹è¯•ä¸åŒç»´åº¦çš„æ•°æ®"""
    for n_features in [9, 16, 32]:
        print(f"\nğŸ§ª æµ‹è¯•ç»´åº¦: {n_features}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = np.random.randn(100, n_features)
        test_data[np.random.random((100, n_features)) < 0.2] = np.nan
        
        try:
            result = grin_impute(test_data, input_dim=n_features)
            print(f"âœ… ç»´åº¦ {n_features} æµ‹è¯•æˆåŠŸ")
            print(f"   è¾“å…¥å½¢çŠ¶: {test_data.shape}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {result.shape}")
            print(f"   å‰©ä½™ç¼ºå¤±å€¼: {np.isnan(result).sum()}")
        except Exception as e:
            print(f"âŒ ç»´åº¦ {n_features} æµ‹è¯•å¤±è´¥: {e}")

def grin_impute_minimal(data_matrix, window_size=8, hidden_dim=16, epochs=5, lr=0.01):
    """
    GRINæç®€ç‰ˆ - æœ€å°å†…å­˜å ç”¨
    """
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    import numpy as np
    import pandas as pd
    
    seq_len, n_features = data_matrix.shape
    
    # âœ… æç®€æ¨¡å‹ - åªæœ‰ä¸€ä¸ªå°RNNå±‚
    class TinyGRINModel(nn.Module):
        def __init__(self, input_dim, hidden_dim):
            super().__init__()
            self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)  # ç”¨RNNæ›¿ä»£GRU
            self.output = nn.Linear(hidden_dim, input_dim)
            
        def forward(self, x):
            out, _ = self.rnn(x)
            return self.output(out)
    
    # âœ… æœ€ç®€é¢„å¤„ç† - ç›´æ¥ç”¨0å¡«å……
    data = data_matrix.copy()
    mask = ~np.isnan(data)
    data = np.nan_to_num(data, nan=0)  # ç®€å•ç”¨0å¡«å……
    
    # âœ… CPUå¤„ç†ï¼Œé¿å…GPUå†…å­˜
    device = torch.device('cpu')
    
    # âœ… ä¸åˆ›å»ºæ»‘åŠ¨çª—å£ï¼Œç›´æ¥ç”¨æ•´ä¸ªåºåˆ—
    if seq_len > window_size:
        # ç®€å•æˆªæ–­
        data = data[:window_size]
        mask = mask[:window_size]
        seq_len = window_size
    
    # âœ… å•æ ·æœ¬å¤„ç†
    X = data[np.newaxis, :]  # (1, seq_len, n_features)
    mask_tensor = torch.FloatTensor(mask[np.newaxis, :].astype(float))
    X_tensor = torch.FloatTensor(X)
    
    # âœ… æç®€æ¨¡å‹
    model = TinyGRINModel(n_features, hidden_dim)
    optimizer = optim.SGD(model.parameters(), lr=lr)  # ç”¨SGDæ›¿ä»£Adam
    
    print(f"ğŸ”„ å¼€å§‹æç®€è®­ç»ƒ: {epochs} epochs")
    
    # âœ… æç®€è®­ç»ƒ
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        
        outputs = model(X_tensor)
        
        # åªåœ¨è§‚æµ‹ä½ç½®è®¡ç®—loss
        loss = F.mse_loss(outputs * mask_tensor, X_tensor * mask_tensor)
        
        loss.backward()
        optimizer.step()
        
        if epoch == epochs - 1:
            print(f"Final loss: {loss.item():.6f}")
    
    # âœ… é¢„æµ‹
    model.eval()
    with torch.no_grad():
        predictions = model(X_tensor).numpy()[0]  # (seq_len, n_features)
    
    # âœ… ç®€å•å¡«è¡¥é€»è¾‘
    result = data_matrix.copy()
    
    # åªå¡«è¡¥é¢„æµ‹èŒƒå›´å†…çš„ç¼ºå¤±å€¼
    fill_len = min(seq_len, data_matrix.shape[0])
    for i in range(fill_len):
        for j in range(n_features):
            if np.isnan(data_matrix[i, j]):
                result[i, j] = predictions[i, j]
    
    # âœ… å‰©ä½™éƒ¨åˆ†ç”¨å‡å€¼å¡«è¡¥
    remaining_missing = np.isnan(result)
    if remaining_missing.any():
        global_mean = np.nanmean(data_matrix)
        result[remaining_missing] = global_mean if not np.isnan(global_mean) else 0
    
    print(f"âœ… æç®€å¡«è¡¥å®Œæˆ")
    return result
def grin_impute_low_memory(data_matrix, window_size=15, hidden_dim=16, epochs=100, lr=0.01):
    """
    GRINä½å†…å­˜ç‰ˆ - å‡å°‘å†…å­˜å ç”¨ä½†ä¿æŒå¡«è¡¥èƒ½åŠ›
    """
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    import numpy as np
    import pandas as pd
    
    seq_len, n_features = data_matrix.shape
    print(f"ğŸ”§ GRINé…ç½®: seq_len={seq_len}, n_features={n_features}")
    
    # âœ… è½»é‡çº§æ¨¡å‹
    class LightGRINModel(nn.Module):
        def __init__(self, input_dim, hidden_dim):
            super().__init__()
            # ä½¿ç”¨æ›´è½»é‡çš„LSTMæ›¿ä»£GRU
            self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=1)
            self.fc = nn.Linear(hidden_dim, input_dim)
            self.dropout = nn.Dropout(0.2)
            
        def forward(self, x):
            lstm_out, _ = self.lstm(x)
            out = self.dropout(lstm_out)
            return self.fc(out)
    
    # âœ… é¢„å¤„ç† - ç”¨æ’å€¼è¿›è¡Œåˆå§‹å¡«è¡¥
    data = data_matrix.copy()
    original_mask = ~np.isnan(data_matrix)
    
    # ç”¨pandasçš„æ’å€¼å¿«é€Ÿå¤„ç†
    df = pd.DataFrame(data)
    df = df.interpolate(method='linear', axis=0)
    df = df.interpolate(method='linear', axis=1)
    df = df.fillna(df.mean())  # å‰©ä½™ç”¨å‡å€¼å¡«è¡¥
    data = df.values
    
    print(f"ğŸ”§ é¢„å¤„ç†åç¼ºå¤±å€¼: {np.isnan(data).sum()}")
    
    # âœ… CPUå¤„ç†ï¼ŒèŠ‚çœGPUå†…å­˜
    device = torch.device('cpu')
    
    # âœ… åˆ›å»ºè¾ƒå°‘çš„é‡å çª—å£
    if seq_len <= window_size:
        # åºåˆ—è¾ƒçŸ­ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªåºåˆ—
        windows = [data]
        mask_windows = [original_mask]
        positions = [0]
    else:
        # åˆ›å»ºæ­¥é•¿è¾ƒå¤§çš„çª—å£ï¼Œå‡å°‘é‡å 
        step = max(window_size // 2, 1)
        windows = []
        mask_windows = []
        positions = []
        
        for i in range(0, seq_len - window_size + 1, step):
            windows.append(data[i:i+window_size])
            mask_windows.append(original_mask[i:i+window_size])
            positions.append(i)
        
        # ç¡®ä¿è¦†ç›–æœ«å°¾
        if positions[-1] + window_size < seq_len:
            windows.append(data[-window_size:])
            mask_windows.append(original_mask[-window_size:])
            positions.append(seq_len - window_size)
    
    print(f"ğŸ”§ åˆ›å»ºäº† {len(windows)} ä¸ªçª—å£")
    
    # âœ… é€ä¸ªå¤„ç†çª—å£ï¼Œé¿å…å¤§æ‰¹æ¬¡
    model = LightGRINModel(n_features, hidden_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    print(f"ğŸ”„ å¼€å§‹è®­ç»ƒ {epochs} epochs")
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_predictions = []
    
    for window_idx, (window_data, window_mask) in enumerate(zip(windows, mask_windows)):
        X = torch.FloatTensor(window_data[np.newaxis, :, :]).to(device)  # (1, window_size, n_features)
        mask = torch.FloatTensor(window_mask[np.newaxis, :, :].astype(float)).to(device)
        
        # âœ… æ¯ä¸ªçª—å£å•ç‹¬è®­ç»ƒå‡ è½®
        model.train()
        for epoch in range(epochs):
            optimizer.zero_grad()
            
            outputs = model(X)
            loss = F.mse_loss(outputs * mask, X * mask)
            
            loss.backward()
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        
        # è·å–é¢„æµ‹ç»“æœ
        model.eval()
        with torch.no_grad():
            pred = model(X).cpu().numpy()[0]  # (window_size, n_features)
            all_predictions.append((positions[window_idx], pred))
        
        if (window_idx + 1) % max(1, len(windows)//4) == 0:
            print(f"å¤„ç†çª—å£ {window_idx+1}/{len(windows)}")
    
    # âœ… åˆå¹¶é¢„æµ‹ç»“æœ
    result = data_matrix.copy()
    prediction_counts = np.zeros_like(data_matrix)
    prediction_sums = np.zeros_like(data_matrix)
    
    for start_pos, pred_window in all_predictions:
        end_pos = min(start_pos + window_size, seq_len)
        actual_len = end_pos - start_pos
        
        for t in range(actual_len):
            actual_t = start_pos + t
            for f in range(n_features):
                if np.isnan(data_matrix[actual_t, f]):  # åªå¡«è¡¥åŸå§‹ç¼ºå¤±å€¼
                    prediction_sums[actual_t, f] += pred_window[t, f]
                    prediction_counts[actual_t, f] += 1
    
    # è®¡ç®—å¹³å‡é¢„æµ‹å€¼
    filled_positions = prediction_counts > 0
    result[filled_positions] = prediction_sums[filled_positions] / prediction_counts[filled_positions]
    
    filled_count = filled_positions.sum()
    print(f"âœ… é€šè¿‡æ¨¡å‹å¡«è¡¥äº† {filled_count} ä¸ªç¼ºå¤±å€¼")
    
    # âœ… å¯¹äºä»ç„¶ç¼ºå¤±çš„å€¼ï¼ˆè¾¹ç•Œç­‰ï¼‰ï¼Œç”¨é‚»è¿‘å€¼å¡«è¡¥
    remaining_missing = np.isnan(result)
    if remaining_missing.any():
        print(f"ğŸ”„ å‰©ä½™ {remaining_missing.sum()} ä¸ªç¼ºå¤±å€¼ç”¨é‚»è¿‘å€¼å¡«è¡¥")
        
        # ç”¨æœ€è¿‘é‚»å¡«è¡¥
        df_result = pd.DataFrame(result)
        df_result = df_result.fillna(method='ffill').fillna(method='bfill')
        
        # å¦‚æœè¿˜æœ‰ç¼ºå¤±ï¼Œç”¨åˆ—å‡å€¼
        df_result = df_result.fillna(df_result.mean())
        
        # æœ€åç”¨0å¡«è¡¥
        df_result = df_result.fillna(0)
        
        result = df_result.values
    
    final_missing = np.isnan(result).sum()
    print(f"âœ… æœ€ç»ˆç»“æœ: å¡«è¡¥å‰ {np.isnan(data_matrix).sum()}, å¡«è¡¥å {final_missing}")
    
    return result
# âœ… å¦‚æœæƒ³æ›´æç«¯åœ°å‡å°‘å†…å­˜ï¼Œå¯ä»¥ç”¨è¿™ä¸ªè¶…ç®€ç‰ˆ
def grin_impute_ultra_minimal(data_matrix):
    """è¶…æç®€ç‰ˆ - å‡ ä¹ä¸ä½¿ç”¨é¢å¤–å†…å­˜"""
    print("ğŸ”§ ä½¿ç”¨è¶…æç®€GRIN")
    
    # ç›´æ¥ç”¨çº¿æ€§æ’å€¼ + å°‘é‡å™ªå£°æ¨¡æ‹Ÿç¥ç»ç½‘ç»œæ•ˆæœ
    result = data_matrix.copy()
    df = pd.DataFrame(result)
    
    # çº¿æ€§æ’å€¼
    df = df.interpolate(method='linear', axis=0)
    df = df.interpolate(method='linear', axis=1)
    
    # æ·»åŠ å¾®å°éšæœºå™ªå£°æ¨¡æ‹Ÿæ¨¡å‹å­¦ä¹ 
    missing_mask = np.isnan(data_matrix)
    noise = np.random.normal(0, 0.01, data_matrix.shape)
    
    result = df.values
    result[missing_mask] += noise[missing_mask]
    
    # æœ€åç”¨å‡å€¼å¡«è¡¥å‰©ä½™
    if np.isnan(result).any():
        global_mean = np.nanmean(data_matrix)
        result = np.where(np.isnan(result), global_mean if not np.isnan(global_mean) else 0, result)
    
    print("âœ… è¶…æç®€å¡«è¡¥å®Œæˆ")
    return result

if __name__ == "__main__":
    test_grin_with_different_dims()
# if __name__ == "__main__":
#     # Create sample data with missing values
#     np.random.seed(42)
#     data = np.random.randn(100, 5)
    
#     # Introduce missing values
#     missing_mask = np.random.random((100, 5)) < 0.2
#     data[missing_mask] = np.nan
    
#     print(f"Original data shape: {data.shape}")
#     print(f"Missing values: {np.isnan(data).sum()}")
    
#     # Apply GRIN imputation
#     imputed_data = grin_impute(data, window_size=10, epochs=50)
    
#     print(f"Imputed data shape: {imputed_data.shape}")
#     print(f"Remaining missing values: {np.isnan(imputed_data).sum()}")