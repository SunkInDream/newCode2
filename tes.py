# import os
# import pandas as pd
# import numpy as np
# import torch
# from multiprocessing import Process, Queue, current_process

# # ========= å…œåº•å‡å€¼å¡«è¡¥ =========
# def mean_impu(mx: np.ndarray) -> np.ndarray:
#     mx = mx.copy().astype(np.float32)
#     col_means = np.nanmean(mx, axis=0)
#     col_means = np.where(np.isnan(col_means), 0.0, col_means)
#     inds = np.where(np.isnan(mx))
#     if inds[0].size > 0:
#         mx[inds] = np.take(col_means, inds[1])
#     return mx

# # ========= KNN å¡«è¡¥ï¼ˆæŒ‰ä½ æä¾›çš„å®ç°ï¼‰=========
# def knn_impu(mx, k=5):
#     from sklearn.impute import KNNImputer

#     mx = mx.copy()
#     original_shape = mx.shape

#     # ç¡®ä¿ k ä¸è¶…è¿‡â€œæ— ç¼ºå¤±è¡Œâ€çš„æ•°é‡
#     non_nan_rows = np.sum(~np.isnan(mx).any(axis=1))
#     if non_nan_rows == 0:
#         return mean_impu(mx)

#     k = min(k, max(1, non_nan_rows - 1))

#     try:
#         imputer = KNNImputer(n_neighbors=k)
#         result = imputer.fit_transform(mx)

#         # ç¡®ä¿è¾“å‡ºå½¢çŠ¶ä¸€è‡´
#         if result.shape != original_shape:
#             result = result[:original_shape[0], :original_shape[1]]
#         return result.astype(np.float32)

#     except Exception as e:
#         print(f"KNN imputation failed: {e}, falling back to mean imputation")
#         return mean_impu(mx).astype(np.float32)

# # ========= å­è¿›ç¨‹ï¼šå¤„ç†é˜Ÿåˆ— =========
# def gpu_worker(file_queue, input_dir, output_dir, gpu_id, use_gpu=True):
#     # MICE/KNN åœ¨ CPU ä¸Šè¿è¡Œï¼Œè¿™é‡Œä»…ä¿ç•™ GPU éš”ç¦»æ¡†æ¶ä»¥ç»Ÿä¸€ç®¡ç†
#     os.environ.setdefault("CUDA_DEVICE_ORDER", "PCI_BUS_ID")
#     os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id) if use_gpu else ""

#     pid = current_process().pid

#     while not file_queue.empty():
#         try:
#             fname = file_queue.get_nowait()
#         except Exception:
#             break

#         out_path = os.path.join(output_dir, fname)
#         # äºŒæ¬¡ä¿æŠ¤ï¼šå·²å­˜åœ¨åˆ™è·³è¿‡
#         if os.path.exists(out_path):
#             print(f"[PID {pid} GPU {gpu_id if use_gpu else 'CPU'}] â© è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶: {fname}")
#             continue

#         try:
#             fpath = os.path.join(input_dir, fname)
#             mx = pd.read_csv(fpath).values.astype(np.float32)

#             filled = knn_impu(mx, k=5)

#             pd.DataFrame(filled).to_csv(out_path, index=False)
#             print(f"[PID {pid} GPU {gpu_id if use_gpu else 'CPU'}] âœ… å¤„ç†å®Œæˆ: {fname}")
#         except Exception as e:
#             print(f"[PID {pid} GPU {gpu_id if use_gpu else 'CPU'}] âŒ å¤„ç†å¤±è´¥: {fname}ï¼Œé”™è¯¯ï¼š{e}")

# # ========= ä¸»è°ƒåº¦ï¼šå¤šè¿›ç¨‹å¹¶è¡Œ =========
# def parallel_knn_impute(input_dir, output_dir, n_processes_per_gpu=2):
#     os.makedirs(output_dir, exist_ok=True)

#     # ä»…æŠŠâ€œç›®æ ‡ä¸­ä¸å­˜åœ¨â€çš„æ–‡ä»¶å…¥é˜Ÿ
#     file_list = [f for f in os.listdir(input_dir) if f.endswith(".csv")]
#     file_list = [f for f in file_list if not os.path.exists(os.path.join(output_dir, f))]

#     file_queue = Queue()
#     for fname in file_list:
#         file_queue.put(fname)

#     num_gpus = torch.cuda.device_count()
#     use_gpu = num_gpus > 0
#     processes = []

#     if use_gpu:
#         total_procs = num_gpus * n_processes_per_gpu
#         print(f"ğŸš€ æ£€æµ‹åˆ° {num_gpus} å¼  GPUï¼›æ¯å¡ {n_processes_per_gpu} è¿›ç¨‹ï¼Œå…± {total_procs} è¿›ç¨‹å¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶")
#         for gpu_id in range(num_gpus):
#             for _ in range(n_processes_per_gpu):
#                 p = Process(target=gpu_worker, args=(file_queue, input_dir, output_dir, gpu_id, True))
#                 p.start()
#                 processes.append(p)
#     else:
#         print(f"âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œæ”¹ä¸º CPU å•è¿›ç¨‹å¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶")
#         p = Process(target=gpu_worker, args=(file_queue, input_dir, output_dir, 0, False))
#         p.start()
#         processes.append(p)

#     for p in processes:
#         p.join()

#     print("âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")

# # ========= å…¥å£ =========
# if __name__ == "__main__":
#     import multiprocessing as mp
#     mp.set_start_method("spawn", force=True)

#     input_dir = "./data/downstreamIII"
#     output_dir = "./data_imputed/knn/III"
#     parallel_knn_impute(input_dir, output_dir, n_processes_per_gpu=2)
# from pathlib import Path

# root = Path("./data_imputed/my_model/III")  # æ”¹æˆä½ çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„
# for p in root.iterdir():
#     if p.is_file() and "_imputed" in p.name:
#         new_path = p.with_name(p.name.replace("_imputed", ""))
#         if new_path.exists():
#             print(f"[å†²çª] è·³è¿‡ï¼š{p.name} -> {new_path.name}ï¼ˆç›®æ ‡å·²å­˜åœ¨ï¼‰")
#             continue
#         p.rename(new_path)
#         print(f"é‡å‘½åï¼š{p.name} -> {new_path.name}")
#!/usr/bin/env python3
# from pathlib import Path

# TARGET_ROWS = 193  # 193 è¡Œï¼šç¬¬ 1 è¡Œæ˜¯åˆ—å + 192 è¡Œæ•°æ®

# def pad_csv_to_193_lines(fp: Path):
#     # è¯»å–å…¨éƒ¨è¡Œï¼ˆé€šå¸¸ CSV è¡Œæ•°ä¸å¤§ï¼›è‹¥ç‰¹åˆ«å¤§å¯æ”¹ä¸ºåªè¯»å–æœ«å°¾ï¼‰
#     try:
#         text = fp.read_text(encoding="utf-8", errors="ignore")
#     except Exception:
#         # è‹¥ç¼–ç ä¸æ˜¯ utf-8ï¼Œå¯æ”¹æˆ fp.read_bytes() å†æ‰‹åŠ¨å¤„ç†
#         print(f"[è·³è¿‡] æ— æ³•è¯»å–ï¼š{fp}")
#         return

#     lines = text.splitlines(keepends=True)  # ä¿ç•™æ¢è¡Œç¬¦ï¼Œä¾¿äºåŸæ ·è¿½åŠ 
#     n = len(lines)

#     if n == 0:
#         print(f"[è·³è¿‡] ç©ºæ–‡ä»¶ï¼š{fp.name}")
#         return

#     if n >= TARGET_ROWS:
#         # å·²è¾¾æ ‡æˆ–è¶…è¿‡ï¼Œä¸å¤„ç†
#         # print(f"[OK] {fp.name} å·²æœ‰ {n} è¡Œ")
#         return

#     if n == 1:
#         # åªæœ‰è¡¨å¤´ï¼Œæ²¡æœ‰æ•°æ®è¡Œï¼Œæ— æ³•å¤åˆ¶â€œæœ€åä¸€è¡Œæ•°æ®â€
#         print(f"[è­¦å‘Š] ä»…æœ‰è¡¨å¤´ï¼Œæ— æ•°æ®å¯å¤åˆ¶ï¼š{fp.name}ï¼ˆå½“å‰ 1 è¡Œï¼‰")
#         return

#     # æœ€åä¸€è¡Œæ•°æ®ï¼ˆä¿ç•™æœ«å°¾æ¢è¡Œï¼›è‹¥æ²¡æœ‰åˆ™è¡¥ä¸€ä¸ªï¼‰
#     last_line = lines[-1]
#     if not last_line.endswith(("\n", "\r")):
#         last_line += "\n"

#     need = TARGET_ROWS - n
#     # è‹¥åŸæ–‡ä»¶æœ€åä¸€è¡Œæ²¡æœ‰æ¢è¡Œï¼Œå…ˆè¡¥ä¸€ä¸ªæ¢è¡Œï¼Œå†å¼€å§‹è¿½åŠ 
#     need_prefix_newline = not lines[-1].endswith(("\n", "\r"))
#     to_append = (("\n" if need_prefix_newline else "") + last_line * need)

#     # è¿½åŠ å†™å›ï¼ˆæ›´å®‰å…¨å¯å†™å…¥ä¸´æ—¶æ–‡ä»¶å†æ›¿æ¢ï¼‰
#     with fp.open("a", encoding="utf-8", newline="") as f:
#         f.write(to_append)

#     print(f"[è¡¥å…¨] {fp.name}: {n} -> {TARGET_ROWS} è¡Œï¼Œå¤åˆ¶æœ€åä¸€è¡Œ {need} æ¬¡")

# def main():
#     folder = Path("./data_imputed/grin/III")  # â† æ”¹æˆä½ çš„ç›®æ ‡æ–‡ä»¶å¤¹
#     for fp in folder.glob("*.csv"):        # è‹¥è¦æ‰€æœ‰æ–‡ä»¶æ”¹ä¸ºï¼šfor fp in folder.iterdir():
#         if fp.is_file():
#             pad_csv_to_193_lines(fp)

# if __name__ == "__main__":
#     main()
# import os
# import numpy as np
# import pandas as pd
# from multiprocessing import Pool
# from sklearn.impute import KNNImputer

# # ====== ç®€å•å¡«è¡¥å‡½æ•° ======
# def zero_impu(mx):
#     return np.where(np.isnan(mx), 0, mx)

# def mean_impu(mx):
#     df = pd.DataFrame(mx)
#     return df.fillna(df.mean()).to_numpy()

# # ====== KNN å¡«è¡¥æ ¸å¿ƒ ======
# def knn_impu(mx, k=3):
#     mx = mx.copy()
#     original_shape = mx.shape

#     # 1. å…ˆæ‰¾å‡ºæ•´åˆ—å…¨ NaN çš„åˆ—
#     all_nan_cols = np.all(np.isnan(mx), axis=0)
#     if all_nan_cols.any():
#         global_mean = np.nanmean(mx)
#         if np.isnan(global_mean):
#             global_mean = 0
#         # è¿™äº›åˆ—å…ˆç”¨å…¨å±€å‡å€¼å¡«
#         mx[:, all_nan_cols] = global_mean

#     # 2. è®¡ç®—æ— ç¼ºå¤±è¡Œæ•°
#     non_nan_rows = np.sum(~np.isnan(mx).any(axis=1))
#     if non_nan_rows == 0:
#         # æ‰€æœ‰è¡Œéƒ½æœ‰ç¼ºå¤±ï¼Œåªèƒ½é€€å›å‡å€¼å¡«è¡¥
#         return mean_impu(mx)

#     k = min(k, max(1, non_nan_rows - 1))
#     try:
#         imputer = KNNImputer(n_neighbors=k)
#         result = imputer.fit_transform(mx)

#         # 3. ç¡®ä¿è¾“å‡ºå½¢çŠ¶ä¸€è‡´
#         if result.shape != original_shape:
#             result = result[:original_shape[0], :original_shape[1]]
#         return result
#     except Exception as e:
#         print(f"KNN imputation failed: {e}, falling back to mean imputation")
#         return mean_impu(mx)

# # ====== å•æ–‡ä»¶å¤„ç† ======
# def process_file(file_info):
#     input_path, output_dir = file_info
#     try:
#         df = pd.read_csv(input_path)
#         mx = df.to_numpy(dtype=float)

#         # æ‰§è¡Œ KNN å¡«è¡¥
#         result = knn_impu(mx, k=3)

#         # ä¿å­˜
#         os.makedirs(output_dir, exist_ok=True)
#         out_file = os.path.join(output_dir, os.path.basename(input_path))
#         pd.DataFrame(result, columns=df.columns).to_csv(out_file, index=False)
#         return f"âœ… Done: {os.path.basename(input_path)}"
#     except Exception as e:
#         return f"âŒ Failed: {os.path.basename(input_path)} -> {e}"

# # ====== å¹¶è¡Œå¤„ç†å‡½æ•° ======
# def parallel_knn_impute(input_dir, output_dir, num_workers=8):
#     files = [os.path.join(input_dir, f) for f in os.listdir(input_dir)
#              if f.endswith(".csv")]
#     tasks = [(f, output_dir) for f in files]

#     with Pool(num_workers) as pool:
#         for res in pool.imap_unordered(process_file, tasks):
#             print(res)

# # ====== ä¸»ç¨‹åºå…¥å£ ======
# if __name__ == "__main__":
#     input_dir = "./data/downstreamIII"
#     output_dir = "./data_imputed/knn/III"
#     parallel_knn_impute(input_dir, output_dir, num_workers=8)
#!/usr/bin/env python3
# from pathlib import Path

# TARGET_ROWS = 193  # 193 è¡Œï¼šç¬¬ 1 è¡Œæ˜¯åˆ—å + 192 è¡Œæ•°æ®

# def pad_csv_to_193_lines(fp: Path):
#     # è¯»å–å…¨éƒ¨è¡Œï¼ˆé€šå¸¸ CSV è¡Œæ•°ä¸å¤§ï¼›è‹¥ç‰¹åˆ«å¤§å¯æ”¹ä¸ºåªè¯»å–æœ«å°¾ï¼‰
#     try:
#         text = fp.read_text(encoding="utf-8", errors="ignore")
#     except Exception:
#         # è‹¥ç¼–ç ä¸æ˜¯ utf-8ï¼Œå¯æ”¹æˆ fp.read_bytes() å†æ‰‹åŠ¨å¤„ç†
#         print(f"[è·³è¿‡] æ— æ³•è¯»å–ï¼š{fp}")
#         return

#     lines = text.splitlines(keepends=True)  # ä¿ç•™æ¢è¡Œç¬¦ï¼Œä¾¿äºåŸæ ·è¿½åŠ 
#     n = len(lines)

#     if n == 0:
#         print(f"[è·³è¿‡] ç©ºæ–‡ä»¶ï¼š{fp.name}")
#         return

#     if n >= TARGET_ROWS:
#         # å·²è¾¾æ ‡æˆ–è¶…è¿‡ï¼Œä¸å¤„ç†
#         # print(f"[OK] {fp.name} å·²æœ‰ {n} è¡Œ")
#         return

#     if n == 1:
#         # åªæœ‰è¡¨å¤´ï¼Œæ²¡æœ‰æ•°æ®è¡Œï¼Œæ— æ³•å¤åˆ¶â€œæœ€åä¸€è¡Œæ•°æ®â€
#         print(f"[è­¦å‘Š] ä»…æœ‰è¡¨å¤´ï¼Œæ— æ•°æ®å¯å¤åˆ¶ï¼š{fp.name}ï¼ˆå½“å‰ 1 è¡Œï¼‰")
#         return

#     # æœ€åä¸€è¡Œæ•°æ®ï¼ˆä¿ç•™æœ«å°¾æ¢è¡Œï¼›è‹¥æ²¡æœ‰åˆ™è¡¥ä¸€ä¸ªï¼‰
#     last_line = lines[-1]
#     if not last_line.endswith(("\n", "\r")):
#         last_line += "\n"

#     need = TARGET_ROWS - n
#     # è‹¥åŸæ–‡ä»¶æœ€åä¸€è¡Œæ²¡æœ‰æ¢è¡Œï¼Œå…ˆè¡¥ä¸€ä¸ªæ¢è¡Œï¼Œå†å¼€å§‹è¿½åŠ 
#     need_prefix_newline = not lines[-1].endswith(("\n", "\r"))
#     to_append = (("\n" if need_prefix_newline else "") + last_line * need)

#     # è¿½åŠ å†™å›ï¼ˆæ›´å®‰å…¨å¯å†™å…¥ä¸´æ—¶æ–‡ä»¶å†æ›¿æ¢ï¼‰
#     with fp.open("a", encoding="utf-8", newline="") as f:
#         f.write(to_append)

#     print(f"[è¡¥å…¨] {fp.name}: {n} -> {TARGET_ROWS} è¡Œï¼Œå¤åˆ¶æœ€åä¸€è¡Œ {need} æ¬¡")

# def main():
#     folder = Path("./data/downstreamIII")  # â† æ”¹æˆä½ çš„ç›®æ ‡æ–‡ä»¶å¤¹
#     for fp in folder.glob("*.csv"):        # è‹¥è¦æ‰€æœ‰æ–‡ä»¶æ”¹ä¸ºï¼šfor fp in folder.iterdir():
#         if fp.is_file():
#             pad_csv_to_193_lines(fp)

# if __name__ == "__main__":
#     main()
# import numpy as np
# import torch
# from sklearn.impute import KNNImputer  # å¤‡ç”¨
# import os

# def mean_impu(mx):
#     import pandas as pd
#     return pd.DataFrame(mx).fillna(pd.DataFrame(mx).mean()).to_numpy()

# def saits_impu(mx, epochs=None, d_model=None, n_layers=None, device=None):
#     from pypots.imputation import SAITS

#     mx = mx.copy()
#     seq_len, n_features = mx.shape
#     total_size = seq_len * n_features

#     # å…¨å±€å‡å€¼
#     global_mean = np.nanmean(mx)
#     if np.isnan(global_mean):
#         global_mean = 0.0

#     # å…¨åˆ—NaNå…ˆå¡«å……
#     all_nan_cols = np.all(np.isnan(mx), axis=0)
#     if all_nan_cols.any():
#         mx[:, all_nan_cols] = global_mean

#     # è‡ªåŠ¨é…ç½®å‚æ•°ï¼ˆæ¯”ä¹‹å‰æ›´è½»ï¼‰
#     if epochs is None:
#         if total_size > 50000:
#             epochs = 10
#             d_model = 16
#             n_layers = 1
#         elif total_size > 10000:
#             epochs = 10
#             d_model = 32
#             n_layers = 1
#         else:
#             epochs = 20
#             d_model = 32
#             n_layers = 1

#     if d_model is None:
#         d_model = min(64, max(16, n_features * 2))

#     if n_layers is None:
#         n_layers = 1

#     try:
#         data_3d = mx[np.newaxis, :, :]

#         saits = SAITS(
#             n_steps=seq_len,
#             n_features=n_features,
#             n_layers=n_layers,
#             d_model=d_model,
#             n_heads=min(2, max(1, d_model // 32)),
#             d_k=max(4, d_model // 8),
#             d_v=max(4, d_model // 8),
#             d_ffn=d_model,
#             dropout=0.1,
#             epochs=epochs,
#             patience=5,
#             batch_size=16,  # é™ä½ batch size
#             device=device or ('cuda' if torch.cuda.is_available() else 'cpu'),
#         )

#         train_set = {"X": data_3d}
#         saits.fit(train_set)
#         imputed_data_3d = saits.impute(train_set)
#         return imputed_data_3d[0]

#     except Exception as e:
#         print(f"SAITS fails: {e}")
#         return mean_impu(mx)
# from multiprocessing import Pool
# import pandas as pd

# def process_file_saits(task):
#     input_path, output_dir, gpu_id = task
#     try:
#         # å›ºå®šè¯¥è¿›ç¨‹åªç”¨ä¸€ä¸ª GPU
#         os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

#         df = pd.read_csv(input_path)
#         mx = df.to_numpy(dtype=float)

#         result = saits_impu(mx)  # è½»é‡ç‰ˆ

#         os.makedirs(output_dir, exist_ok=True)
#         out_file = os.path.join(output_dir, os.path.basename(input_path))
#         pd.DataFrame(result, columns=df.columns).to_csv(out_file, index=False)
#         return f"[GPU {gpu_id}] Done: {os.path.basename(input_path)}"
#     except Exception as e:
#         return f"[GPU {gpu_id}] Failed: {os.path.basename(input_path)} -> {e}"

# def parallel_saits_impute(input_dir, output_dir, num_gpus=2, workers_per_gpu=1):
#     files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(".csv")]
#     tasks = []
#     gpu_list = list(range(num_gpus))
#     for i, f in enumerate(files):
#         gpu_id = gpu_list[i % num_gpus]
#         tasks.append((f, output_dir, gpu_id))

#     with Pool(num_gpus * workers_per_gpu) as pool:
#         for res in pool.imap_unordered(process_file_saits, tasks):
#             print(res)
# if __name__ == "__main__":
#     input_dir = "./data/downstreamIII"
#     output_dir = "./data_imputed/saits/III"
#     parallel_saits_impute(input_dir, output_dir, num_gpus=2, workers_per_gpu=1)
# ===============================
# é€‰é¡¹ 2: æ¯ä¸ªæ–‡ä»¶å•ç‹¬è®¡ç®— std
# ===============================
import os
import numpy as np
import pandas as pd

# æ–‡ä»¶å¤¹è·¯å¾„
folder = "./data/air"


all_values = []

for fname in os.listdir(folder):
    if fname.endswith(".csv"):
        fpath = os.path.join(folder, fname)
        df = pd.read_csv(fpath)
        all_values.append(df.values.flatten())

all_values = np.concatenate(all_values)
global_mean = np.mean(all_values)
print(f"å…¨å±€ mean = {global_mean}")