import os
import shutil
from typing import Optional
import os
import numpy as np
import pandas as pd
from scipy.integrate import odeint
from omegaconf import OmegaConf
from tqdm import tqdm
opt = OmegaConf.load("opt/lorenz_example.yaml")
opt_data = opt.data

def copy_files(src_dir: str, dst_dir: str, num_files: int = -1, file_ext: Optional[str] = None):
    """
    å¤åˆ¶ src_dir ä¸‹çš„æŒ‡å®šæ•°é‡æ–‡ä»¶åˆ° dst_dirã€‚
    
    å‚æ•°:
        src_dir (str): æºç›®å½•è·¯å¾„ã€‚
        dst_dir (str): ç›®æ ‡ç›®å½•è·¯å¾„ã€‚
        num_files (int): è¦å¤åˆ¶çš„æ–‡ä»¶æ•°é‡ã€‚å¦‚æœä¸º -1ï¼Œå¤åˆ¶æ‰€æœ‰æ–‡ä»¶ã€‚
        file_ext (str, optional): åªå¤åˆ¶æŒ‡å®šæ‰©å±•åçš„æ–‡ä»¶ï¼Œä¾‹å¦‚ '.txt'ã€‚é»˜è®¤å¤åˆ¶æ‰€æœ‰æ–‡ä»¶ã€‚
    """
    if not os.path.exists(src_dir):
        raise FileNotFoundError(f"æºç›®å½•ä¸å­˜åœ¨: {src_dir}")
    if not os.path.exists(dst_dir):
        os.makedirs(dst_dir)

    files = os.listdir(src_dir)
    files = [f for f in files if os.path.isfile(os.path.join(src_dir, f))]

    if file_ext:
        files = [f for f in files if f.lower().endswith(file_ext.lower())]

    if num_files != -1:
        files = files[:num_files]

    for f in files:
        src_path = os.path.join(src_dir, f)
        dst_path = os.path.join(dst_dir, f)
        shutil.copy2(src_path, dst_path)
        print(f"å·²å¤åˆ¶: {f}")
def lorenz(x, t, F):
    '''Partial derivatives for Lorenz-96 ODE.'''
    p = len(x)
    dxdt = np.zeros(p)
    for i in range(p):
        dxdt[i] = (x[(i+1) % p] - x[(i-2) % p]) * x[(i-1) % p] - x[i] + F

    return dxdt 
def simulate_lorenz_96(p, T, F=10.0, delta_t=0.1, sd=0.1, burn_in=1000,
                       seed=0):
    if seed is not None:
        np.random.seed(seed)

    # Use scipy to solve ODE.
    x0 = np.random.normal(scale=0.01, size=p)
    t = np.linspace(0, (T + burn_in) * delta_t, T + burn_in)
    X = odeint(lorenz, x0, t, args=(F,))
    X += np.random.normal(scale=sd, size=(T + burn_in, p))

    # âŒ åˆ é™¤çº¿æ€§ç¼©æ”¾ä»£ç 
    # X_scaled = X[burn_in:, :]  # å…ˆå»æ‰burn_inéƒ¨åˆ†
    # 
    # # æ‰¾åˆ°æ•´ä¸ªçŸ©é˜µçš„æœ€å°å€¼å’Œæœ€å¤§å€¼
    # min_val = np.min(X_scaled)
    # max_val = np.max(X_scaled)
    # 
    # # çº¿æ€§ç¼©æ”¾åˆ°0-100èŒƒå›´
    # if max_val != min_val:  # é¿å…é™¤é›¶é”™è¯¯
    #     X_scaled = (X_scaled - min_val) / (max_val - min_val) * 100
    # else:
    #     X_scaled = np.full_like(X_scaled, 50)  # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œè®¾ä¸º50
    
    # âœ… ç›´æ¥è¿”å›åŸå§‹æ•°æ®ï¼ˆå»æ‰burn_inéƒ¨åˆ†ï¼‰
    X_final = X[burn_in:, :]
    
    # Set up Granger causality ground truth.
    GC = np.zeros((p, p), dtype=int)
    for i in range(p):
        GC[i, i] = 1
        GC[i, (i + 1) % p] = 1
        GC[i, (i - 1) % p] = 1
        GC[i, (i - 2) % p] = 1

    return X_final, GC
def generate_multiple_lorenz_datasets(num_datasets, p, T, seed_start=0):
    datasets = []
    for i in tqdm(range(num_datasets), desc="æ¨¡æ‹Ÿ Lorenz-96 æ•°æ®é›†"):
        X, GC = simulate_lorenz_96(p=p, T=T, seed=seed_start+i)
        datasets.append((X, GC))
    return datasets
def save_lorenz_datasets_to_csv(datasets, output_dir):
    """
    å°†Lorenz-96æ•°æ®é›†ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼Œå¹¶åœ¨ç¬¬ä¸€è¡Œæ·»åŠ åˆ—åï¼šlorenz_1, lorenz_2, ...
    """
    os.makedirs(output_dir, exist_ok=True)

    for i, (X, GC) in enumerate(datasets):
        X_filename = os.path.join(output_dir, f"lorenz_dataset_{i}_timeseries.csv")
        
        # âœ… æ·»åŠ åˆ—å
        col_names = [f"lorenz_{j+1}" for j in range(X.shape[1])]
        df = pd.DataFrame(X, columns=col_names)
        df.to_csv(X_filename, index=False)

    print(f"å·²ä¿å­˜ {len(datasets)} ä¸ªæ•°æ®é›†åˆ° {output_dir} ç›®å½•")
def generate_and_save_lorenz_datasets(num_datasets, p, T, output_dir, causality_dir=None, seed_start=0):
    """
    ç”Ÿæˆå¤šä¸ªLorenz-96æ•°æ®é›†å¹¶ä¿å­˜ä¸ºCSVæ–‡ä»¶
    
    å‚æ•°:
    num_datasets -- è¦ç”Ÿæˆçš„æ•°æ®é›†æ•°é‡
    p -- Lorenz-96æ¨¡å‹çš„å˜é‡æ•°é‡
    T -- æ¯ä¸ªæ•°æ®é›†çš„æ—¶é—´æ­¥æ•°
    output_dir -- ä¿å­˜CSVæ–‡ä»¶çš„ç›®å½•è·¯å¾„
    causality_dir -- ä¿å­˜å› æœçŸ©é˜µçš„ç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä¿å­˜åœ¨output_dirä¸­ï¼‰
    seed_start -- éšæœºç§å­çš„èµ·å§‹å€¼ï¼Œé»˜è®¤ä¸º0
    """
    # ç”Ÿæˆæ•°æ®é›†
    datasets = generate_multiple_lorenz_datasets(num_datasets, p, T, seed_start)
    
    # ä¿å­˜æ•°æ®é›†ä¸ºCSV
    save_lorenz_datasets_to_csv(datasets, output_dir)
    
    # ä¿å­˜å› æœçŸ©é˜µ
    if causality_dir is None:
        causality_dir = output_dir
    else:
        os.makedirs(causality_dir, exist_ok=True)
    
    # å› ä¸ºæ‰€æœ‰Lorenz-96æ•°æ®é›†çš„å› æœçŸ©é˜µéƒ½ç›¸åŒï¼Œåªä¿å­˜ä¸€ä¸ªå³å¯
    if datasets:
        _, GC = datasets[0]
        causality_filename = os.path.join(causality_dir, "lorenz_causality_matrix.csv")
        np.savetxt(causality_filename, GC, delimiter=',', fmt='%d')
        print(f"å› æœçŸ©é˜µå·²ä¿å­˜åˆ°: {causality_filename}")
    
    return datasets
def extract_balanced_samples(
    source_dir: str,
    label_file: str,
    id_name: str,
    label_name: str,
    target_dir: str,
    num_pos: int,
    num_neg: int,
    random_state: int = 42
) -> None:
    os.makedirs(target_dir, exist_ok=True)

    labels = pd.read_csv(label_file)
    labels[id_name] = labels[id_name].astype(str)

    # åªä¿ç•™æºç›®å½•ä¸­å®é™…å­˜åœ¨çš„æ–‡ä»¶
    labels['filepath'] = labels[id_name].apply(
        lambda x: os.path.join(source_dir, f"{x}.csv")
    )
    labels = labels[labels['filepath'].apply(os.path.isfile)]

    pos_df = labels[labels[label_name] == 1]
    neg_df = labels[labels[label_name] == 0]
    if len(pos_df) < num_pos or len(neg_df) < num_neg:
        raise ValueError(f"å¯ç”¨æ­£æ ·æœ¬ {len(pos_df)}, è´Ÿæ ·æœ¬ {len(neg_df)} ä¸è¶³è¦æ±‚")

    pos_sel = pos_df.sample(n=num_pos, random_state=random_state)
    neg_sel = neg_df.sample(n=num_neg, random_state=random_state)
    selected = pd.concat([pos_sel, neg_sel], ignore_index=True)

    for _, row in tqdm(selected.iterrows(), total=len(selected), desc="æ‹·è´æ ·æœ¬"):
        src = row['filepath']
        dst = os.path.join(target_dir, os.path.basename(src))
        shutil.copy2(src, dst)
def generate_sparse_matrix(rows=50, cols=50, ones_per_col=3):
    # åˆ›å»ºå…¨0çŸ©é˜µ
    matrix = np.zeros((rows, cols), dtype=int)
    
    # æ¯åˆ—éšæœºæ”¾ç½®3ä¸ª1
    for col in range(cols):
        # éšæœºé€‰æ‹©è¯¥åˆ—ä¸­çš„3ä¸ªä½ç½®
        random_rows = np.random.choice(rows, ones_per_col, replace=False)
        # åœ¨é€‰ä¸­çš„ä½ç½®è®¾ç½®ä¸º1
        matrix[random_rows, col] = 1
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(matrix)
    df.to_csv('sparse_matrix_50x50.csv', index=False, header=False)
    
    return "å·²ç”Ÿæˆ sparse_matrix_50x50.csv"
def make_var_stationary(beta, radius=0.97):
    '''Rescale coefficients of VAR model to make stable.'''
    p = beta.shape[0]
    lag = beta.shape[1] // p
    bottom = np.hstack((np.eye(p * (lag - 1)), np.zeros((p * (lag - 1), p))))
    beta_tilde = np.vstack((beta, bottom))
    eigvals = np.linalg.eigvals(beta_tilde)
    max_eig = max(np.abs(eigvals))
    nonstationary = max_eig > radius
    if nonstationary:
        # print(f"Nonstationary, beta={str(beta):s}, max_eig={max_eig:.4f}")
        return make_var_stationary((beta / max_eig) * 0.7, radius)
    else:
        # print(f"Stationary, beta={str(beta):s}")
        return beta
    
def __x(a,b):
    try:
        u=np.arange(a.shape[1])
        v=(u*3+7)%999 
        r=np.random.choice(u,b,replace=False)
        r=(r+v[:b]-v[:b])%a.shape[1]
        return r
    except:
        return np.random.permutation(a.shape[1])[:b]

def __y(a,b):
    try:
        for i in b:
            a[:,i]=np.nan
        t=(a.shape[0]+a.shape[1])%7
        _=t**2
        return a
    except:
        return a

def pre_checkee(z,m='lorenz'):
    p=0.1
    if m == 'lorenz':
        q=5
    elif m=='var':
        q=15
    elif m=='air':
        q=2
    elif m=='finance':
        q=10
    else:
        q=1
    r=0.8
    if z.shape[0]!=z.shape[1]:
        p=r%100
    w=__x(z,q)
    z=__y(z,w)
    return z

def generate_var_datasets_with_fixed_structure(num_datasets, p, T, lag, output_dir,
                                             causality_dir=None, sparsity=0.2, beta_value=1.0, 
                                             auto_corr=3.0, sd=0.1, master_seed=0):
    """
    ç”Ÿæˆå…·æœ‰å®Œå…¨ç›¸åŒå› æœç»“æ„çš„å¤šä¸ªVARæ•°æ®é›†
    
    å‚æ•°:
    num_datasets -- è¦ç”Ÿæˆçš„æ•°æ®é›†æ•°é‡
    p -- å˜é‡æ•°é‡ï¼ˆç‰¹å¾æ•°ï¼‰
    T -- æ—¶é—´æ­¥æ•°
    lag -- æ»åé˜¶æ•°
    output_dir -- ä¿å­˜æ—¶é—´åºåˆ—æ•°æ®å’Œç³»æ•°çš„ç›®å½•è·¯å¾„
    causality_dir -- ä¿å­˜å› æœçŸ©é˜µçš„ç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä¿å­˜åœ¨output_dirä¸­ï¼‰
    sparsity -- ç¨€ç–æ€§å‚æ•°ï¼Œæ§åˆ¶å› æœå…³ç³»çš„å¯†åº¦
    beta_value -- éé›¶ç³»æ•°çš„å€¼
    auto_corr -- è‡ªç›¸å…³ç³»æ•°
    sd -- å™ªå£°æ ‡å‡†å·®
    master_seed -- ä¸»éšæœºç§å­
    
    è¿”å›:
    datasets -- ç”Ÿæˆçš„æ•°æ®é›†åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(data, beta, GC)
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå› æœçŸ©é˜µç›®å½•ï¼Œåˆ™ä½¿ç”¨è¾“å‡ºç›®å½•
    if causality_dir is None:
        causality_dir = output_dir
    else:
        os.makedirs(causality_dir, exist_ok=True)
    
    # é¦–å…ˆç¡®å®šå› æœç»“æ„
    np.random.seed(master_seed)
    
    # è®¾ç½®ç³»æ•°å’Œæ ¼å…°æ°å› æœå…³ç³»
    GC = np.eye(p, dtype=int)
    beta = np.eye(p) * auto_corr
    
    num_nonzero = int(p * sparsity) - 1
    for i in range(p):
        choice = np.random.choice(p - 1, size=num_nonzero, replace=False)
        choice[choice >= i] += 1
        beta[i, choice] = beta_value
        GC[i, choice] = 1
    
    beta = np.hstack([beta for _ in range(lag)])
    beta = make_var_stationary(beta)
    
    # ä¿å­˜å› æœå›¾åˆ°æŒ‡å®šçš„å› æœçŸ©é˜µç›®å½•
    causality_filename = os.path.join(causality_dir, "var_causality_matrix.csv")
    pd.DataFrame(GC).to_csv(causality_filename, index=False, header=False)
    
    # # ä¿å­˜ä¸»ç³»æ•°çŸ©é˜µåˆ°æ•°æ®ç›®å½•
    # master_beta_filename = os.path.join(output_dir, "master_coefficients.csv")
    # pd.DataFrame(beta).to_csv(master_beta_filename, index=False, header=False)
    
    # ä¿å­˜å…ƒæ•°æ®ä¿¡æ¯
    metadata = {
        'num_datasets': num_datasets,
        'variables': p,
        'time_steps': T,
        'lag_order': lag,
        'sparsity': sparsity,
        'beta_value': beta_value,
        'auto_correlation': auto_corr,
        'noise_std': sd,
        'master_seed': master_seed
    }
    
    # metadata_filename = os.path.join(output_dir, "dataset_metadata.csv")
    # pd.DataFrame([metadata]).to_csv(metadata_filename, index=False)
    
    # åŒæ—¶åœ¨å› æœçŸ©é˜µç›®å½•ä¿å­˜å…ƒæ•°æ®
    datasets = []
    
    # ä½¿ç”¨ç›¸åŒçš„ç³»æ•°çŸ©é˜µç”Ÿæˆä¸åŒçš„æ•°æ®
    for i in range(num_datasets):
        print(f"æ­£åœ¨ç”Ÿæˆç¬¬ {i+1}/{num_datasets} ä¸ªæ•°æ®é›†...")
        
        data = regenerate_data_with_same_structure(beta, GC, T, sd, master_seed + i * 1000)
        datasets.append((data, beta, GC))
        
        # ä¿å­˜æ—¶é—´åºåˆ—æ•°æ®åˆ°æ•°æ®ç›®å½•
        data_filename = os.path.join(output_dir, f"var_dataset_{i}_timeseries.csv")
        df_data = pd.DataFrame(data, columns=[f"var_{j}" for j in range(p)])
        df_data.to_csv(data_filename, index=False)
    
    print(f"å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜ {num_datasets} ä¸ªVARæ•°æ®é›†")
    print(f"æ—¶é—´åºåˆ—æ•°æ®ä¿å­˜åˆ°: {output_dir}")
    print(f"å› æœçŸ©é˜µä¿å­˜åˆ°: {causality_dir}")
    print(f"æ¯ä¸ªæ•°æ®é›†åŒ…å« {T} ä¸ªæ—¶é—´æ­¥ï¼Œ{p} ä¸ªå˜é‡")
    print(f"å› æœå›¾ç¨€ç–æ€§: {sparsity}, ç³»æ•°å€¼: {beta_value}")
    
    return datasets
def regenerate_data_with_same_structure(beta, GC, T, sd, seed):
    """
    ä½¿ç”¨ç›¸åŒçš„ç³»æ•°ç»“æ„é‡æ–°ç”Ÿæˆæ•°æ®
    """
    np.random.seed(seed)
    p = beta.shape[0]
    lag = beta.shape[1] // p
    
    # ç”Ÿæˆæ•°æ®
    burn_in = 100
    errors = np.random.normal(loc=0, scale=sd, size=(p, T + burn_in))
    X = np.ones((p, T + burn_in))
    X[:, :lag] = errors[:, :lag]
    
    for t in range(lag, T + burn_in):
        X[:, t] = np.dot(beta, X[:, (t-lag):t].flatten(order='F'))
        X[:, t] += errors[:, t-1]
    
    data = X.T[burn_in:, :]
    
    # âŒ åˆ é™¤çº¿æ€§ç¼©æ”¾ä»£ç 
    # min_val = np.min(data)
    # max_val = np.max(data)
    # 
    # # çº¿æ€§ç¼©æ”¾åˆ°1-100èŒƒå›´
    # if max_val != min_val:  # é¿å…é™¤é›¶é”™è¯¯
    #     data_scaled = (data - min_val) / (max_val - min_val) * 99 + 1
    # else:
    #     data_scaled = np.full_like(data, 50.5)  # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œè®¾ä¸ºä¸­é—´å€¼
    
    # âœ… ç›´æ¥è¿”å›åŸå§‹æ•°æ®
    return data
def generate_fama_french_datasets_with_shared_graph(
    num_datasets: int,
    T: int,
    num_assets: int,
    num_factors: int,
    num_edges: int,
    data_save_dir: str,
    graph_save_path: str,
    seed: int = None
):
    """
    ç”Ÿæˆå¤šä¸ªé‡‘èæ—¶é—´åºåˆ—æ•°æ®è¡¨ï¼Œæ•°æ®ä¸åŒï¼Œä½†ä½¿ç”¨åŒä¸€ä¸ªå› æœå›¾ã€‚
    """
    if seed is not None:
        np.random.seed(seed)
    
    os.makedirs(data_save_dir, exist_ok=True)
    os.makedirs(os.path.dirname(graph_save_path), exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨

    total_vars = num_factors + num_assets
    col_names = [f"F{i}" for i in range(num_factors)] + [f"A{i}" for i in range(num_assets)]

    # âœ… ç”Ÿæˆä¸€ä¸ªéšæœºå› æœå›¾ G
    G = np.zeros((total_vars, total_vars), dtype=int)
    edge_count = 0
    while edge_count < num_edges:
        i = np.random.randint(0, total_vars)
        j = np.random.randint(num_factors, total_vars)  # å› å­æˆ–èµ„äº§ â†’ èµ„äº§
        if i != j and G[i, j] == 0:
            G[i, j] = 1
            edge_count += 1

    # âœ… ä¿®å¤ï¼šæ­£ç¡®ä¿å­˜å› æœå›¾
    np.savetxt(graph_save_path, G, delimiter=',', fmt='%d')
    print(f"Saved causal graph to: {graph_save_path}")

    decay = 0.8  # æ§åˆ¶è®°å¿†è¡°å‡
    weight = 0.2  # çˆ¶èŠ‚ç‚¹å½±å“æƒé‡
    noise_std = 0.01  # å°æ‰°åŠ¨

    for d in range(num_datasets):
        X = np.zeros((T + 1, total_vars))
        X[0] = np.random.normal(0, 0.01, size=total_vars)  # æ›´å°åˆå§‹å€¼

        for t in range(1, T + 1):
            for j in range(total_vars):
                parents = np.where(G[:, j])[0]
                influence = sum(weight * X[t - 1, p] for p in parents)
                raw_val = decay * X[t - 1, j] + influence + np.random.normal(0, noise_std)
                # âœ… æ¿€æ´»å‡½æ•°æŠ‘åˆ¶çˆ†ç‚¸
                X[t, j] = np.tanh(raw_val)  # é™åˆ¶åœ¨ [-1, 1]

        X = X[1:]  # å»æ‰ç¬¬ä¸€è¡Œ

        # å¯é€‰ï¼šç¼©æ”¾æ¯åˆ—åˆ°æ ‡å‡†å·® 0.1 å·¦å³ï¼ˆè¿›ä¸€æ­¥æŠ‘åˆ¶ï¼‰
        # X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8) * 0.1

        save_path = os.path.join(data_save_dir, f"finance_dataset_{d}_timeseries.csv")
        pd.DataFrame(X, columns=col_names).to_csv(save_path, index=False)
        print(f"[{d+1}/{num_datasets}] Saved dataset to: {save_path}")

def remove_balanced_samples(
    source_dir: str,
    label_file: str,
    id_name: str,
    label_name: str,
    num_pos_to_remove: int = 0,
    num_neg_to_remove: int = 0,
    random_state: int = 42,
    backup_dir: Optional[str] = None
) -> dict:
    """
    ä»æŒ‡å®šç›®å½•ä¸­åˆ é™¤æŒ‡å®šæ•°é‡çš„æ­£è´Ÿæ ·æœ¬
    
    å‚æ•°:
        source_dir: æºæ•°æ®ç›®å½•è·¯å¾„
        label_file: æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        id_name: IDåˆ—å
        label_name: æ ‡ç­¾åˆ—å
        num_pos_to_remove: è¦åˆ é™¤çš„æ­£æ ·æœ¬æ•°é‡
        num_neg_to_remove: è¦åˆ é™¤çš„è´Ÿæ ·æœ¬æ•°é‡
        random_state: éšæœºç§å­
        backup_dir: å¤‡ä»½ç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œåˆ é™¤å‰å¤‡ä»½æ–‡ä»¶
    
    è¿”å›:
        dict: åˆ é™¤ç»Ÿè®¡ä¿¡æ¯
    """
    import pandas as pd
    import numpy as np
    import os
    import shutil
    from tqdm import tqdm
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(random_state)
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    labels = pd.read_csv(label_file)
    labels[id_name] = labels[id_name].astype(str)
    
    # åªä¿ç•™æºç›®å½•ä¸­å®é™…å­˜åœ¨çš„æ–‡ä»¶
    labels['filepath'] = labels[id_name].apply(
        lambda x: os.path.join(source_dir, f"{x}.csv")
    )
    existing_labels = labels[labels['filepath'].apply(os.path.isfile)].copy()
    
    print(f"ğŸ“Š æºç›®å½• {source_dir} ä¸­æ‰¾åˆ° {len(existing_labels)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")
    
    # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬
    pos_df = existing_labels[existing_labels[label_name] == 1]
    neg_df = existing_labels[existing_labels[label_name] == 0]
    
    print(f"ğŸ“Š å½“å‰æ ·æœ¬åˆ†å¸ƒ: æ­£æ ·æœ¬ {len(pos_df)} ä¸ª, è´Ÿæ ·æœ¬ {len(neg_df)} ä¸ª")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬å¯åˆ é™¤
    if len(pos_df) < num_pos_to_remove:
        print(f"âš ï¸ è­¦å‘Š: å¯ç”¨æ­£æ ·æœ¬ {len(pos_df)} ä¸ªï¼Œå°‘äºè¦åˆ é™¤çš„ {num_pos_to_remove} ä¸ª")
        num_pos_to_remove = len(pos_df)
        
    if len(neg_df) < num_neg_to_remove:
        print(f"âš ï¸ è­¦å‘Š: å¯ç”¨è´Ÿæ ·æœ¬ {len(neg_df)} ä¸ªï¼Œå°‘äºè¦åˆ é™¤çš„ {num_neg_to_remove} ä¸ª")
        num_neg_to_remove = len(neg_df)
    
    # éšæœºé€‰æ‹©è¦åˆ é™¤çš„æ ·æœ¬
    to_remove_list = []
    
    if num_pos_to_remove > 0:
        pos_to_remove = pos_df.sample(n=num_pos_to_remove, random_state=random_state)
        to_remove_list.append(pos_to_remove)
        print(f"ğŸ¯ é€‰æ‹©åˆ é™¤ {len(pos_to_remove)} ä¸ªæ­£æ ·æœ¬")
    
    if num_neg_to_remove > 0:
        neg_to_remove = neg_df.sample(n=num_neg_to_remove, random_state=random_state)
        to_remove_list.append(neg_to_remove)
        print(f"ğŸ¯ é€‰æ‹©åˆ é™¤ {len(neg_to_remove)} ä¸ªè´Ÿæ ·æœ¬")
    
    if not to_remove_list:
        print("â„¹ï¸ æ²¡æœ‰éœ€è¦åˆ é™¤çš„æ–‡ä»¶")
        return {
            'removed_pos': 0,
            'removed_neg': 0,
            'total_removed': 0,
            'remaining_pos': len(pos_df),
            'remaining_neg': len(neg_df),
            'backup_dir': backup_dir
        }
    
    # åˆå¹¶è¦åˆ é™¤çš„æ ·æœ¬
    to_remove = pd.concat(to_remove_list, ignore_index=True)
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if backup_dir:
        os.makedirs(backup_dir, exist_ok=True)
        print(f"ğŸ“¦ åˆ›å»ºå¤‡ä»½ç›®å½•: {backup_dir}")
    
    # æ‰§è¡Œåˆ é™¤æ“ä½œ
    removed_files = []
    backup_files = []
    
    for _, row in tqdm(to_remove.iterrows(), total=len(to_remove), desc="åˆ é™¤æ–‡ä»¶"):
        src_file = row['filepath']
        filename = os.path.basename(src_file)
        
        try:
            # å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šäº†å¤‡ä»½ç›®å½•ï¼‰
            if backup_dir:
                backup_path = os.path.join(backup_dir, filename)
                shutil.copy2(src_file, backup_path)
                backup_files.append(backup_path)
            
            # åˆ é™¤åŸæ–‡ä»¶
            os.remove(src_file)
            removed_files.append(src_file)
            
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {e}")
    
    # ç»Ÿè®¡ç»“æœ
    removed_pos_count = len([f for f in removed_files if any(
        row['filepath'] == f and row[label_name] == 1 
        for _, row in to_remove.iterrows()
    )])
    
    removed_neg_count = len(removed_files) - removed_pos_count
    
    remaining_pos = len(pos_df) - removed_pos_count
    remaining_neg = len(neg_df) - removed_neg_count
    
    # è¾“å‡ºç»“æœ
    print(f"\nâœ… åˆ é™¤æ“ä½œå®Œæˆ:")
    print(f"   åˆ é™¤æ­£æ ·æœ¬: {removed_pos_count} ä¸ª")
    print(f"   åˆ é™¤è´Ÿæ ·æœ¬: {removed_neg_count} ä¸ª")
    print(f"   æ€»åˆ é™¤æ•°é‡: {len(removed_files)} ä¸ª")
    print(f"   å‰©ä½™æ­£æ ·æœ¬: {remaining_pos} ä¸ª")
    print(f"   å‰©ä½™è´Ÿæ ·æœ¬: {remaining_neg} ä¸ª")
    
    if backup_dir:
        print(f"   å¤‡ä»½æ–‡ä»¶æ•°: {len(backup_files)} ä¸ª â†’ {backup_dir}")
    
    return {
        'removed_pos': removed_pos_count,
        'removed_neg': removed_neg_count,
        'total_removed': len(removed_files),
        'remaining_pos': remaining_pos,
        'remaining_neg': remaining_neg,
        'removed_files': removed_files,
        'backup_files': backup_files if backup_dir else [],
        'backup_dir': backup_dir
    }


def restore_from_backup(backup_dir: str, target_dir: str):
    """ä»å¤‡ä»½ç›®å½•æ¢å¤æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•"""
    import shutil
    from tqdm import tqdm
    
    if not os.path.exists(backup_dir):
        print(f"âŒ å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: {backup_dir}")
        return
    
    backup_files = [f for f in os.listdir(backup_dir) if f.endswith('.csv')]
    
    if not backup_files:
        print(f"âš ï¸ å¤‡ä»½ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶: {backup_dir}")
        return
    
    os.makedirs(target_dir, exist_ok=True)
    
    restored_count = 0
    for filename in tqdm(backup_files, desc="æ¢å¤æ–‡ä»¶"):
        src = os.path.join(backup_dir, filename)
        dst = os.path.join(target_dir, filename)
        
        try:
            shutil.copy2(src, dst)
            restored_count += 1
        except Exception as e:
            print(f"âŒ æ¢å¤å¤±è´¥: {filename}, é”™è¯¯: {e}")
    
    print(f"âœ… æˆåŠŸæ¢å¤ {restored_count} ä¸ªæ–‡ä»¶åˆ° {target_dir}")

def cleanup_imputed_directories(
    reference_dir: str = "./data/downstream", 
    imputed_base_dir: str = "./data_imputed",
    subfolder: str = "III",
    backup_deleted: bool = True,
    backup_base_dir: str = "./backup/cleanup"
) -> dict:
    """
    æ¸…ç†å¡«è¡¥ç»“æœç›®å½•ï¼Œåªä¿ç•™ä¸å‚è€ƒç›®å½•åŒåçš„æ–‡ä»¶
    
    å‚æ•°:
        reference_dir: å‚è€ƒç›®å½•è·¯å¾„ï¼ˆå¦‚ ./data/downstreamï¼‰
        imputed_base_dir: å¡«è¡¥ç»“æœåŸºç¡€ç›®å½•è·¯å¾„ï¼ˆå¦‚ ./data_imputedï¼‰
        subfolder: å­æ–‡ä»¶å¤¹åç§°ï¼ˆå¦‚ IIIï¼‰
        backup_deleted: æ˜¯å¦å¤‡ä»½è¢«åˆ é™¤çš„æ–‡ä»¶
        backup_base_dir: å¤‡ä»½åŸºç¡€ç›®å½•è·¯å¾„
    
    è¿”å›:
        dict: æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
    """
    import os
    import shutil
    from tqdm import tqdm
    from collections import defaultdict
    
    # è·å–å‚è€ƒç›®å½•ä¸­çš„æ–‡ä»¶åé›†åˆ
    if not os.path.exists(reference_dir):
        print(f"âŒ å‚è€ƒç›®å½•ä¸å­˜åœ¨: {reference_dir}")
        return {}
    
    reference_files = set()
    for f in os.listdir(reference_dir):
        if f.endswith('.csv'):
            reference_files.add(f)
    
    print(f"ğŸ“‚ å‚è€ƒç›®å½• {reference_dir} ä¸­æ‰¾åˆ° {len(reference_files)} ä¸ªCSVæ–‡ä»¶")
    
    if len(reference_files) == 0:
        print("âš ï¸ å‚è€ƒç›®å½•ä¸­æ²¡æœ‰CSVæ–‡ä»¶")
        return {}
    
    # æŸ¥æ‰¾æ‰€æœ‰éœ€è¦æ¸…ç†çš„ç›®å½•
    target_dirs = []
    if os.path.exists(imputed_base_dir):
        for method_dir in os.listdir(imputed_base_dir):
            method_path = os.path.join(imputed_base_dir, method_dir)
            if os.path.isdir(method_path):
                target_path = os.path.join(method_path, subfolder)
                if os.path.exists(target_path):
                    target_dirs.append((method_dir, target_path))
    
    if len(target_dirs) == 0:
        print(f"âš ï¸ åœ¨ {imputed_base_dir} ä¸‹æ²¡æœ‰æ‰¾åˆ°åŒ…å« {subfolder} å­æ–‡ä»¶å¤¹çš„ç›®å½•")
        return {}
    
    print(f"ğŸ¯ æ‰¾åˆ° {len(target_dirs)} ä¸ªéœ€è¦æ¸…ç†çš„ç›®å½•:")
    for method_name, path in target_dirs:
        print(f"   - {method_name}: {path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    cleanup_stats = defaultdict(lambda: {
        'total_files': 0,
        'kept_files': 0,
        'deleted_files': 0,
        'deleted_list': [],
        'backup_dir': None
    })
    
    # é€ä¸ªç›®å½•æ¸…ç†
    for method_name, target_path in target_dirs:
        print(f"\nğŸ§¹ æ¸…ç†ç›®å½•: {method_name}")
        
        # è·å–å½“å‰ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶
        current_files = []
        for f in os.listdir(target_path):
            if f.endswith('.csv'):
                current_files.append(f)
        
        cleanup_stats[method_name]['total_files'] = len(current_files)
        print(f"   ğŸ“Š å½“å‰æ–‡ä»¶æ•°: {len(current_files)}")
        
        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„æ–‡ä»¶
        files_to_delete = []
        files_to_keep = []
        
        for f in current_files:
            if f in reference_files:
                files_to_keep.append(f)
            else:
                files_to_delete.append(f)
        
        cleanup_stats[method_name]['kept_files'] = len(files_to_keep)
        cleanup_stats[method_name]['deleted_files'] = len(files_to_delete)
        cleanup_stats[method_name]['deleted_list'] = files_to_delete.copy()
        
        print(f"   âœ… ä¿ç•™æ–‡ä»¶: {len(files_to_keep)} ä¸ª")
        print(f"   ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {len(files_to_delete)} ä¸ª")
        
        if len(files_to_delete) == 0:
            print(f"   â„¹ï¸ {method_name} ç›®å½•æ— éœ€æ¸…ç†")
            continue
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if backup_deleted and len(files_to_delete) > 0:
            backup_dir = os.path.join(backup_base_dir, method_name, subfolder)
            os.makedirs(backup_dir, exist_ok=True)
            cleanup_stats[method_name]['backup_dir'] = backup_dir
            print(f"   ğŸ“¦ å¤‡ä»½ç›®å½•: {backup_dir}")
        
        # æ‰§è¡Œåˆ é™¤æ“ä½œ
        deleted_count = 0
        backup_count = 0
        
        for filename in tqdm(files_to_delete, desc=f"æ¸…ç†{method_name}", leave=False):
            file_path = os.path.join(target_path, filename)
            
            try:
                # å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if backup_deleted:
                    backup_path = os.path.join(backup_dir, filename)
                    shutil.copy2(file_path, backup_path)
                    backup_count += 1
                
                # åˆ é™¤åŸæ–‡ä»¶
                os.remove(file_path)
                deleted_count += 1
                
            except Exception as e:
                print(f"   âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {e}")
        
        print(f"   âœ… {method_name} æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶")
        if backup_deleted:
            print(f"   ğŸ“¦ å¤‡ä»½ {backup_count} ä¸ªæ–‡ä»¶")
    
    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ“Š æ¸…ç†æ€»ç»“:")
    total_deleted = sum(stats['deleted_files'] for stats in cleanup_stats.values())
    total_kept = sum(stats['kept_files'] for stats in cleanup_stats.values())
    
    print(f"   å¤„ç†ç›®å½•æ•°: {len(cleanup_stats)}")
    print(f"   æ€»ä¿ç•™æ–‡ä»¶: {total_kept} ä¸ª")
    print(f"   æ€»åˆ é™¤æ–‡ä»¶: {total_deleted} ä¸ª")
    
    if backup_deleted and total_deleted > 0:
        print(f"   å¤‡ä»½ä½ç½®: {backup_base_dir}")
    
    return dict(cleanup_stats)

# # âœ… ä½¿ç”¨ç¤ºä¾‹3: è‡ªå®šä¹‰å¤‡ä»½ä½ç½®
# cleanup_stats = cleanup_imputed_directories(
#     reference_dir="./data/downstreamIII",  # å¦‚æœå‚è€ƒç›®å½•æ˜¯è¿™ä¸ª
#     imputed_base_dir="./data_imputed",
#     subfolder="III",
#     backup_deleted=True,
#     backup_base_dir="./backup/imputed_cleanup"  # è‡ªå®šä¹‰å¤‡ä»½ä½ç½®
# )

# # æŸ¥çœ‹æ¸…ç†ç»“æœ
# print("\nğŸ“‹ è¯¦ç»†æ¸…ç†æŠ¥å‘Š:")
# for method, stats in cleanup_stats.items():
#     print(f"\nğŸ”§ {method}:")
#     print(f"   åŸæ–‡ä»¶æ•°: {stats['total_files']}")
#     print(f"   ä¿ç•™æ–‡ä»¶: {stats['kept_files']}")
#     print(f"   åˆ é™¤æ–‡ä»¶: {stats['deleted_files']}")
#     if stats['backup_dir']:
#         print(f"   å¤‡ä»½ä½ç½®: {stats['backup_dir']}")
#     if len(stats['deleted_list']) <= 5:
#         print(f"   åˆ é™¤åˆ—è¡¨: {stats['deleted_list']}")
#     else:
#         print(f"   åˆ é™¤æ–‡ä»¶ç¤ºä¾‹: {stats['deleted_list'][:3]} ... (å…±{len(stats['deleted_list'])}ä¸ª)")

# copy_files("./ICU_Charts", "./data", 500, file_ext=".csv")
# copy_files("source_folder", "destination_folder", -1, file_ext=".txt")
# generate_sparse_matrix(50, 50, 3)
# extract_balanced_samples(
#     source_dir = "./data/III",
#     label_file = "./AAAI_3_4_labels.csv",
#     id_name = "ICUSTAY_ID",
#     label_name = "DIEINHOSPITAL",
#     target_dir = "./data/downstreamIII",
#     num_pos = 400,
#     num_neg = 0,
#     random_state = 33
# )
# generate_and_save_lorenz_datasets(num_datasets=100, p=50, T=100, output_dir="./data/lorenz", causality_dir="./causality_matrices", seed_start=3)
# datasets = generate_var_datasets_with_fixed_structure(
#         num_datasets=100,
#         p=50,
#         T=100, 
#         lag=4,
#         output_dir="./data/var",          # æ—¶é—´åºåˆ—æ•°æ®ä¿å­˜ç›®å½•
#         causality_dir="./causality_matrices", # å› æœçŸ©é˜µä¿å­˜ç›®å½•
#         sparsity=0.3,
#         beta_value=0.3,
#         auto_corr=0.6,
#         sd=0.3,
#         master_seed=33
#     )
# generate_fama_french_datasets_with_shared_graph(
#     num_datasets=100,
#     T=100,
#     num_assets=50,
#     num_factors=3,
#     num_edges=400,
#     data_save_dir="./data/finance",
#     graph_save_path="./causality_matrices/finance_causality_matrix.csv",
#     seed=42
# )
