import os
import torch
import random
import numpy as np
import pandas as pd
from tqdm import tqdm
# import tensorflow as tf
import multiprocessing as mp 
from models_TCDF import *
import torch.nn.functional as F
from pygrinder import block_missing
from sklearn.cluster import KMeans
from models_TCDF import *
from baseline import *
from models_downstream import *
from multiprocessing import Process, Queue

def FirstProcess(matrix, threshold=0.8):
    df = pd.DataFrame(matrix)
    for column in df.columns:
        col_data = df[column]
        if col_data.isna().all():
            df[column] = -1
        else:
            non_nan_data = col_data.dropna()
            value_counts = non_nan_data.value_counts()
            mode_value = value_counts.index[0]
            mode_count = value_counts.iloc[0]
            if mode_count >= threshold * len(non_nan_data):
                df[column] = col_data.fillna(mode_value)
    return df.values

def SecondProcess(matrix, perturbation_prob=0.1, perturbation_scale=0.1):
    df_copy = pd.DataFrame(matrix)
    for column in df_copy.columns:
        series = df_copy[column]
        missing_mask = series.isna()

        if not missing_mask.any():
            continue  # å¦‚æœæ²¡æœ‰ç¼ºå¤±å€¼ï¼Œè·³è¿‡è¯¥åˆ—
        missing_segments = []
        start_idx = None

        # æŸ¥æ‰¾ç¼ºå¤±å€¼çš„è¿ç»­æ®µ
        for i, is_missing in enumerate(missing_mask):
            if is_missing and start_idx is None:
                start_idx = i
            elif not is_missing and start_idx is not None:
                missing_segments.append((start_idx, i - 1))
                start_idx = None
        if start_idx is not None:
            missing_segments.append((start_idx, len(missing_mask) - 1))

        # å¯¹æ¯ä¸ªç¼ºå¤±æ®µè¿›è¡Œå¡«è¡¥
        for start, end in missing_segments:
            left_value, right_value = None, None
            left_idx, right_idx = start - 1, end + 1

            # æ‰¾åˆ°å‰åæœ€è¿‘çš„éç¼ºå¤±å€¼
            while left_idx >= 0 and np.isnan(series.iloc[left_idx]):
                left_idx -= 1
            if left_idx >= 0:
                left_value = series.iloc[left_idx]

            while right_idx < len(series) and np.isnan(series.iloc[right_idx]):
                right_idx += 1
            if right_idx < len(series):
                right_value = series.iloc[right_idx]

            # å¦‚æœå‰åéƒ½æ²¡æœ‰éç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‡å€¼å¡«å……
            if left_value is None and right_value is None:
                fill_value = series.dropna().mean()
                df_copy.loc[missing_mask, column] = fill_value
                continue

            # å¦‚æœåªæœ‰ä¸€ä¸ªæ–¹å‘æœ‰éç¼ºå¤±å€¼ï¼Œä½¿ç”¨å¦ä¸€ä¸ªæ–¹å‘çš„å€¼å¡«å……
            if left_value is None:
                left_value = right_value
            elif right_value is None:
                right_value = left_value

            # ä½¿ç”¨ç­‰å·®æ•°åˆ—å¡«è¡¥ç¼ºå¤±å€¼
            segment_length = end - start + 1
            step = (right_value - left_value) / (segment_length + 1)
            values = [left_value + step * (i + 1) for i in range(segment_length)]

            # æ·»åŠ æ‰°åŠ¨
            value_range = np.abs(right_value - left_value) or (np.abs(left_value) * 0.1 if left_value != 0 else 1.0)
            for i in range(len(values)):
                if random.random() < perturbation_prob:
                    perturbation = random.uniform(-1, 1) * perturbation_scale * value_range
                    values[i] += perturbation

            # å°†å¡«è¡¥åçš„å€¼èµ‹å›æ•°æ®æ¡†
            for i, value in enumerate(values):
                df_copy.iloc[start + i, df_copy.columns.get_loc(column)] = value

    return df_copy.values.astype(np.float32)

def initial_process(matrix, threshold=0.8, perturbation_prob=0.1, perturbation_scale=0.1):
    matrix = FirstProcess(matrix, threshold)
    matrix = SecondProcess(matrix, perturbation_prob, perturbation_scale)
    return matrix

def impute(original, causal_matrix, model_params, epochs=150, lr=0.02, gpu_id=None):
    if gpu_id is not None and torch.cuda.is_available():
        device = torch.device(f'cuda:{gpu_id}')
    else:
        device = torch.device('cpu')
    original_copy = original.copy()
    # é˜¶æ®µ1: å¡«è¡¥ç©ºåˆ— + é«˜é‡å¤åˆ—
    first_stage_initial_filled = FirstProcess(original_copy)
    # é˜¶æ®µ2: æ•°å€¼æ‰°åŠ¨å¢å¼º
    initial_filled = SecondProcess(first_stage_initial_filled)

    mask = (~np.isnan(first_stage_initial_filled)).astype(int)
    sequence_len, total_features = initial_filled.shape
    final_filled = initial_filled.copy()

    for target in range(total_features):
        # é€‰æ‹©å› æœç‰¹å¾
        inds = list(np.where(causal_matrix[:, target] == 1)[0])
        if target not in inds:
            inds.append(target)
        else:
            inds.remove(target)
            inds.append(target)
        inds = inds[:3] + [target]  # ä¿ç•™

        # æ„é€ æ»åç›®æ ‡å˜é‡
        target_shifted = np.roll(initial_filled[:, target], 1)
        target_shifted[0] = 0.0
        x_data = np.concatenate([initial_filled[:, inds], target_shifted[:, None]], axis=1)

        x = torch.tensor(x_data.T[np.newaxis, ...], dtype=torch.float32).to(device)
        y = torch.tensor(initial_filled[:, target][np.newaxis, :, None], dtype=torch.float32).to(device)
        m = torch.tensor((mask[:, target] == 1)[np.newaxis, :, None], dtype=torch.float32).to(device)

        # æ„å»ºæ¨¡å‹
        input_dim = x.shape[1]
        model = ADDSTCN(target, input_size=input_dim, cuda=(device != torch.device('cpu')), **model_params).to(device)

        optimizer = torch.optim.Adam(model.parameters(), lr=lr)

        # ç¼–è¯‘åŠ é€Ÿ
        if hasattr(torch, 'compile'):
            try:
                model = torch.compile(model)
            except:
                pass

        # è®­ç»ƒ
        for epoch in range(1, epochs + 1):
            model.train()
            optimizer.zero_grad()
            pred = model(x)
            loss = F.mse_loss(pred * m, y * m)
            loss.backward()
            optimizer.step()

        # æ¨ç†
        model.eval()
        with torch.no_grad():
            out = model(x).squeeze().cpu().numpy()
            to_fill = np.where(mask[:, target] == 0)
            to_fill_filtered = to_fill[0]
            if len(to_fill_filtered) > 0:
                final_filled[to_fill_filtered, target] = out[to_fill_filtered]

    return final_filled


def impute_single_file(file_path, causal_matrix, model_params, epochs=100, lr=0.01, gpu_id=None):
    """å•æ–‡ä»¶å¡«è¡¥å‡½æ•°ï¼Œç”¨äºè¿›ç¨‹æ± """
    # # è®¾ç½®GPU
    # if gpu_id != 'cpu' and torch.cuda.is_available():
    #     torch.cuda.set_device(gpu_id)
    #     device = torch.device(f'cuda:{gpu_id}')
    # else:
    #     device = torch.device('cpu')
    
    # è¯»å–æ•°æ®
    data = pd.read_csv(file_path).values.astype(np.float32)
    filename = os.path.basename(file_path)
        
    # è°ƒç”¨ä¼˜åŒ–åçš„imputeå‡½æ•°
    result = impute(data, causal_matrix, model_params, epochs=epochs, lr=lr, gpu_id=gpu_id)
    return filename, result

def parallel_impute(file_path, causal_matrix, model_params, epochs=150, lr=0.02, simultaneous_per_gpu=2, max_workers=None):
    """ä½¿ç”¨è¿›ç¨‹æ± çš„å¹¶è¡Œå¡«è¡¥"""
    # è·å–æ–‡ä»¶åˆ—è¡¨
    file_list = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith(".csv")]
    
    # ç¡®å®šå·¥ä½œè¿›ç¨‹æ•°å’ŒGPUåˆ†é…
    num_gpus = torch.cuda.device_count()
    if num_gpus == 0:
        max_workers = max_workers or os.cpu_count()
        gpu_ids = ['cpu'] * len(file_list)
    else:
        # æ¯ä¸ªGPUè¿è¡Œsimultaneous_per_gpuä¸ªè¿›ç¨‹ä»¥æé«˜åˆ©ç”¨ç‡
        max_workers = max_workers or (num_gpus * simultaneous_per_gpu)
        gpu_ids = [i % num_gpus for i in range(len(file_list))]
    
    print(f"ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶")
    
    # å‡†å¤‡å‚æ•°åˆ—è¡¨
    args_list = [(file_path, causal_matrix, model_params, epochs, lr, gpu_id) 
                 for file_path, gpu_id in zip(file_list, gpu_ids)]
    
    # å¹¶è¡Œæ‰§è¡Œ
    with Pool(processes=max_workers) as pool:
        results = list(tqdm(
            pool.starmap(impute_single_file, args_list),
            total=len(file_list),
            desc="æ‰¹é‡å¡«è¡¥ä¸­",
            ncols=80
        ))
    
    # ä¿å­˜ç»“æœ
    os.makedirs("./data_imputed/my_model/mimic-iii", exist_ok=True)
    successful_results = []
    
    for filename, result in results:
        successful_results.append(result)
        pd.DataFrame(result).to_csv(f"./data_imputed/my_model/mimic-iii/{filename}", index=False)
    
    print(f"æˆåŠŸå¡«è¡¥ {len(successful_results)}/{len(file_list)} ä¸ªæ–‡ä»¶")
    return successful_results

def agregate(initial_filled_array, n_cluster):
    # Step 1: æ¯ä¸ªæ ·æœ¬æŒ‰åˆ—å–å‡å€¼ï¼Œæ„é€ èšç±»è¾“å…¥
    data = np.array([np.nanmean(x, axis=0) for x in initial_filled_array])

    # Step 2: KMeans èšç±»
    km = KMeans(n_clusters=n_cluster, n_init=10, random_state=0)
    labels = km.fit_predict(data)

    # Step 3: é€ç±»æ‰¾ä»£è¡¨æ ·æœ¬ï¼Œå¸¦è¿›åº¦æ¡
    idx_arr = []
    for k in tqdm(range(n_cluster), desc="é€‰æ‹©æ¯ç°‡ä»£è¡¨æ ·æœ¬"):
        idxs = np.where(labels == k)[0]
        if len(idxs) == 0:
            continue
        cluster_data = data[idxs]
        dists = np.linalg.norm(cluster_data - km.cluster_centers_[k], axis=1)
        best_idx = idxs[np.argmin(dists)]
        idx_arr.append(int(best_idx))

    return idx_arr

def causal_discovery(original_matrix_arr, n_cluster=5, isStandard=False, standard_cg=None,
                     params={
                         'layers': 6,
                         'kernel_size': 6,
                         'dilation_c': 4,
                         'optimizername': 'Adam',
                         'lr': 0.02,
                         'epochs': 100,
                         'significance': 1.2,
                     }):
    if isStandard:
        if standard_cg is None:
            raise ValueError("standard_cg must be provided when isStandard is True")
        return pd.read_csv(standard_cg).values

    # Step 1: é¢„å¤„ç†
    initial_matrix_arr = original_matrix_arr.copy()
    for i in tqdm(range(len(initial_matrix_arr)), desc="é¢„å¤„ç†æ ·æœ¬"):
        initial_matrix_arr[i] = initial_process(initial_matrix_arr[i])

    # Step 2: èšç±»å¹¶æå–ä»£è¡¨æ ·æœ¬
    idx_arr = agregate(initial_matrix_arr, n_cluster)
    data_list = [initial_matrix_arr[idx] for idx in idx_arr]
    params_list = [params] * len(data_list)

    # Step 3: å¤š GPU å¹¶è¡Œå› æœå‘ç°
    results = parallel_compute_causal_matrices(data_list, params_list)

    # Step 4: æ±‡æ€»ç»“æœ
    cg_total = None
    for matrix in results:
        if matrix is None:
            continue
        if cg_total is None:
            cg_total = matrix.copy()
        else:
            cg_total += matrix

    if cg_total is None:
        raise RuntimeError("æ‰€æœ‰ä»»åŠ¡éƒ½å¤±è´¥ï¼Œæœªèƒ½å¾—åˆ°æœ‰æ•ˆçš„å› æœçŸ©é˜µ")

    # # Step 5: é€‰ Top-3 æ„å»ºæœ€ç»ˆå› æœå›¾
    # np.fill_diagonal(cg_total, 0)
    # new_matrix = np.zeros_like(cg_total)
    # for col in range(cg_total.shape[1]):
    #     col_values = cg_total[:, col]
    #     if np.count_nonzero(col_values) < 3:
    #         new_matrix[:, col] = 1
    #     else:
    #         top3 = np.argsort(col_values)[-3:]
    #         new_matrix[top3, col] = 1
       # Step 5: é€‰ Top-5 æ„å»ºæœ€ç»ˆå› æœå›¾
    np.fill_diagonal(cg_total, 0)
    new_matrix = np.zeros_like(cg_total)
    for col in range(cg_total.shape[1]):
        col_values = cg_total[:, col]
        if np.count_nonzero(col_values) < 5:
            new_matrix[:, col] = 1
        else:
            top5 = np.argsort(col_values)[-5:]
            new_matrix[top5, col] = 1
    return new_matrix

# ================================
# 1. å•æ–‡ä»¶è¯„ä¼°å‡½æ•°
# ================================
def mse_evaluate_single_file(mx, causal_matrix, gpu_id=0, device=None):
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    gt = mx.copy()
    X = block_missing(mx[np.newaxis, ...], factor=0.2, block_width=15, block_len=10)[0]

    def mse(a, b): 
        a = torch.as_tensor(a, dtype=torch.float32, device=device) 
        b = torch.as_tensor(b, dtype=torch.float32, device=device) 
        return F.mse_loss(a, b).item()

    res = {}
    res['my_model'] = mse(
        impute(X, causal_matrix,
               model_params={'num_levels':10, 'kernel_size': 8, 'dilation_c': 2},
               epochs=150, lr=0.02, gpu_id=gpu_id),
        gt
    )

    if gpu_id == 0:  # åªåœ¨å¤„ç†ç¬¬ä¸€ä¸ªæ•°æ®è¡¨æ—¶ä¿å­˜
        zero_result = zero_impu(X)
        my_model_result = impute(X, causal_matrix,
                           model_params={'num_levels':10, 'kernel_size': 8, 'dilation_c': 2},
                           epochs=150, lr=0.02, gpu_id=gpu_id)
        pd.DataFrame(gt).to_csv("gt_matrix.csv", index=False)
        pd.DataFrame(my_model_result).to_csv("my_model_matrix.csv", index=False)
        pd.DataFrame(zero_result).to_csv("zero_impu_matrix.csv", index=False)
        print("âœ… å·²ä¿å­˜ gt_matrix.csv, my_model_matrix.csv, zero_impu_matrix.csv")

    def is_reasonable_mse(mse_value, threshold=10000.0):
        """æ£€æŸ¥MSEå€¼æ˜¯å¦åˆç†"""
        return (not np.isnan(mse_value) and 
                not np.isinf(mse_value) and 
                0 <= mse_value <= threshold)

    baseline = [
        ('zero_impu', zero_impu), ('mean_impu', mean_impu),
        ('median_impu', median_impu), ('mode_impu', mode_impu),
        ('random_impu', random_impu), ('knn_impu', knn_impu),
        ('ffill_impu', ffill_impu), ('bfill_impu', bfill_impu),
        ('miracle_impu', miracle_impu), ('saits_impu', saits_impu),
        ('timemixerpp_impu', timemixerpp_impu), 
        ('tefn_impu', tefn_impu),
    ]

    for name, fn in baseline:
        try:
            print(f"å¼€å§‹æ‰§è¡Œ {name}...")
            result = fn(X)
            
            if result is None:
                print(f"âŒ {name}: è¿”å›äº† None")
                res[name] = float('nan')
            elif not isinstance(result, np.ndarray):
                print(f"âŒ {name}: è¿”å›ç±»å‹é”™è¯¯ {type(result)}")
                res[name] = float('nan')
            elif result.shape != X.shape:
                print(f"âŒ {name}: å½¢çŠ¶ä¸åŒ¹é… {result.shape} vs {X.shape}")
                res[name] = float('nan')
            else:
                # æ£€æŸ¥å¡«è¡¥ç»“æœæ˜¯å¦åŒ…å«å¼‚å¸¸å€¼
                if np.any(np.isnan(result)) or np.any(np.isinf(result)):
                    print(f"âŒ {name}: å¡«è¡¥ç»“æœåŒ…å« NaN æˆ– Inf")
                    res[name] = float('nan')
                elif np.any(np.abs(result) > 1e6):  # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤§å€¼
                    print(f"âŒ {name}: å¡«è¡¥ç»“æœåŒ…å«å¼‚å¸¸å¤§å€¼ (max: {np.max(np.abs(result)):.2e})")
                    res[name] = float('nan')
                else:
                    mse_value = mse(result, gt)
                    
                    # æ£€æŸ¥MSEæ˜¯å¦åˆç†
                    if is_reasonable_mse(mse_value, threshold=10000.0):
                        res[name] = mse_value
                        print(f"âœ… {name}: {mse_value:.6f}")
                    else:
                        print(f"âŒ {name}: MSEå¼‚å¸¸ ({mse_value:.2e})")
                        res[name] = float('nan')
                
        except Exception as e:
            print(f"âŒ {name}: æ‰§è¡Œå¤±è´¥ - {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            res[name] = float('nan')

    print(f"æ‰€æœ‰ç»“æœ: {res}")

    # for name, fn in baseline:
    #     res[name] = mse(fn(X), gt)

    return res

# ================================
# 2. ç”¨äº Pool çš„åŒ…è£…å‡½æ•°ï¼ˆæ¯ä¸ªä»»åŠ¡ï¼‰
# ================================
def worker_wrapper(args):
    idx, mx, causal_matrix, gpu_id = args
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    
    # ç¡®ä¿PyTorchä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
    if torch.cuda.is_available():
        torch.cuda.set_device(0)  # åœ¨å­è¿›ç¨‹ä¸­ï¼ŒGPU 0å°±æ˜¯åˆ†é…ç»™çš„GPU
        device = torch.device('cuda:0')
    else:
        device = torch.device('cpu')
    
    # ä¼ é€’æ­£ç¡®çš„è®¾å¤‡
    res = mse_evaluate_single_file(mx, causal_matrix, gpu_id=0, device=device)  # â† è¿™é‡Œæ”¹ä¸º0
    return idx, res

# ================================
# 3. å¹¶è¡Œè°ƒåº¦å‡½æ•°ï¼ˆè¿›ç¨‹æ± å®ç°ï¼‰
# ================================
def parallel_mse_evaluate(res_list, causal_matrix, simultaneous_per_gpu=3):
    num_gpus = torch.cuda.device_count()

    if num_gpus == 0:
        print("[INFO] æ²¡æœ‰å¯ç”¨ GPUï¼Œä½¿ç”¨ CPU é¡ºåºè¯„ä¼°")
        all_res = [mse_evaluate_single_file(x, causal_matrix)
                   for x in tqdm(res_list, desc="CPU")]
        return {k: float(np.mean([d[k] for d in all_res]))
                for k in all_res[0]}

    # æ ¹æ® GPU æ•°é‡å’Œæ¯å¼ å¡å¯å¹¶è¡Œä»»åŠ¡æ•°ï¼Œè®¾ç½®è¿›ç¨‹æ± å¤§å°
    max_workers = num_gpus * simultaneous_per_gpu
    print(f"[INFO] ä½¿ç”¨ {num_gpus} ä¸ª GPUï¼Œæ¯ä¸ª GPU æœ€å¤šå¹¶è¡Œ {simultaneous_per_gpu} ä¸ªä»»åŠ¡ï¼Œæ€»è¿›ç¨‹æ•°: {max_workers}")

    # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…å¯¹åº”çš„ GPU
    gpu_ids = [i % num_gpus for i in range(len(res_list))]
    args_list = [(i, res_list[i], causal_matrix, gpu_ids[i]) for i in range(len(res_list))]

    with mp.Pool(processes=max_workers) as pool:
        results = list(tqdm(pool.imap(worker_wrapper, args_list), total=len(args_list), desc="Allâ€‘tasks"))

    results.sort(key=lambda x: x[0])  # æ¢å¤é¡ºåº
    only_result_dicts = [res for _, res in results]

    avg = {}
    for k in only_result_dicts[0]:
        values = [r[k] for r in only_result_dicts]
        valid_values = [v for v in values if not np.isnan(v)]
        
        if len(valid_values) > 0:
            avg[k] = float(np.nanmean(values))
            print(f"ğŸ“Š {k}: {len(valid_values)}/{len(values)} ä¸ªæœ‰æ•ˆå€¼ï¼Œå¹³å‡: {avg[k]:.6f}")
        else:
            avg[k] = float('nan')
            print(f"âŒ {k}: æ‰€æœ‰å€¼éƒ½æ˜¯ NaN")
    pd.DataFrame([{'Method': k, 'Average_MSE': v} for k, v in avg.items()]) \
        .to_csv("mse_evaluation_results.csv", index=False)

    return avg
