{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06111fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_impute import *\n",
    "from pygrinder import block_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/lorenz/lorenz_dataset_0_timeseries.csv\",header=None)\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35727148",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = pd.read_csv('./causality_matrices/lorenz_causality_matrix.csv', header=None)\n",
    "cg = cg.values\n",
    "model_params = {\n",
    "        'num_levels': 10,\n",
    "        'kernel_size': 8,\n",
    "        'dilation_c': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = block_missing(data[np.newaxis,...],factor=0.1, block_len=3, block_width=3)\n",
    "data = data[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3939b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed = impute(data, cg, model_params)\n",
    "data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd675b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_miracle.py\n",
    "import numpy as np\n",
    "from baseline import miracle_impu\n",
    "\n",
    "# ÂàõÂª∫ÊµãËØïÊï∞ÊçÆ\n",
    "test_data = np.random.randn(100, 10)\n",
    "test_data[np.random.random((100, 10)) < 0.1] = np.nan\n",
    "\n",
    "print(\"ÊµãËØïÊï∞ÊçÆÂΩ¢Áä∂:\", test_data.shape)\n",
    "print(\"Áº∫Â§±ÂÄºÊï∞Èáè:\", np.isnan(test_data).sum())\n",
    "\n",
    "try:\n",
    "    result = miracle_impu(test_data)\n",
    "    print(\"MIRACLEÁªìÊûúÂΩ¢Áä∂:\", result.shape if result is not None else \"None\")\n",
    "    print(\"MIRACLEÁªìÊûúÁ±ªÂûã:\", type(result))\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"Â°´Ë°•ÂêéÁº∫Â§±ÂÄº:\", np.isnan(result).sum())\n",
    "    else:\n",
    "        print(\"‚ùå MIRACLEËøîÂõû‰∫ÜNone\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå MIRACLEÊµãËØïÂ§±Ë¥•: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf579a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ËØªÂèñ CSV Êñá‰ª∂\n",
    "zero_matrix = pd.read_csv(\"zero_impu_matrix.csv\").values  # shape: (T, N)\n",
    "\n",
    "# ÊûÑÈÄ†Êé©Á†ÅÁü©ÈòµÔºö0 ÁöÑ‰ΩçÁΩÆ‰∏∫ 0ÔºåÂÖ∂‰Ωô‰∏∫ 1\n",
    "mask_matrix = np.where(zero_matrix == 0, 0, 1)\n",
    "\n",
    "# ‰øùÂ≠òÊñ∞Áü©ÈòµÂà∞ CSV\n",
    "pd.DataFrame(mask_matrix).to_csv(\"zero_impu_mask.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa10119",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv('./train_result.csv').values\n",
    "gt = pd.read_csv('./gt_matrix.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbe677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = 1 - mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred*mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64647142",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt*mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = F.mse_loss(torch.tensor(pred*mask_matrix), torch.tensor(gt*mask_matrix))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f44c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "tmp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "tmp_file.close()\n",
    "os.remove(tmp_file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a02438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ÂéüÂßãÁõÆÂΩïÂíåÁõÆÊ†áÁõÆÂΩïË∑ØÂæÑ\n",
    "src_dir = './data/III'        # ÊõøÊç¢‰∏∫‰Ω†ÁöÑÊ∫êÁõÆÂΩïË∑ØÂæÑ\n",
    "dst_dir = './data/mimic-iii'   # ÊõøÊç¢‰∏∫‰Ω†ÁöÑÁõÆÊ†áÁõÆÂΩïË∑ØÂæÑ\n",
    "\n",
    "# ÂàõÂª∫ÁõÆÊ†áÁõÆÂΩïÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºâ\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# Ëé∑ÂèñÊ∫êÁõÆÂΩï‰∏≠ÁöÑÊâÄÊúâÊñá‰ª∂ÂêçÔºàÊåâÂêçÁß∞ÊéíÂ∫èÔºåÂèØÊîπ‰∏∫Êåâ‰øÆÊîπÊó∂Èó¥Á≠âÔºâ\n",
    "all_files = sorted(f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f)))\n",
    "\n",
    "# ÈÄâÊã©Ââç100‰∏™Êñá‰ª∂\n",
    "files_to_move = all_files[:100]\n",
    "\n",
    "# ÁßªÂä®Êñá‰ª∂\n",
    "for fname in files_to_move:\n",
    "    shutil.move(os.path.join(src_dir, fname), os.path.join(dst_dir, fname))\n",
    "\n",
    "print(f\"Â∑≤ÊàêÂäüÁßªÂä® {len(files_to_move)} ‰∏™Êñá‰ª∂Âà∞ {dst_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9813c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from baseline import *\n",
    "from models_impute import *\n",
    "# ÊåáÂÆöÁõÆÂΩï\n",
    "data_dir = \"./data/III\"  # ÊõøÊç¢‰∏∫‰Ω†ÁöÑÁõÆÂΩïË∑ØÂæÑ\n",
    "\n",
    "# Ëé∑ÂèñÁõÆÂΩï‰∏ãÁ¨¨‰∏Ä‰∏™ CSV Êñá‰ª∂\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "assert csv_files, \"ÁõÆÂΩï‰∏≠Ê≤°ÊúâÊâæÂà∞ CSV Êñá‰ª∂\"\n",
    "csv_path = os.path.join(data_dir, csv_files[0])\n",
    "\n",
    "# ËØªÂèñÊï∞ÊçÆ\n",
    "df = pd.read_csv(csv_path)\n",
    "mx = df.values.astype(np.float32)\n",
    "\n",
    "# ËÆ∞ÂΩïÂéüÂßãÁº∫Â§±‰ΩçÁΩÆ\n",
    "nan_pos = np.argwhere(np.isnan(mx))\n",
    "\n",
    "# Â°´Ë°•Áº∫Â§±ÂÄº\n",
    "imputed_mx = timesnet_impu(mx,)\n",
    "\n",
    "# ÊâìÂç∞Â°´Ë°•ÂâçÂêéÁöÑÂÄºÔºà‰ªÖÁº∫Â§±‰ΩçÁΩÆÔºâ\n",
    "print(f\"Â°´Ë°•ÂâçÂêéÂØπÊØîÔºà‰ªÖÁº∫Â§±‰ΩçÁΩÆÔºâÔºö{imputed_mx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import sys\n",
    "\n",
    "for _, name, _ in pkgutil.iter_modules():\n",
    "    if 'tsde' in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygrinder import mcar, mar_logistic, mnar_x\n",
    "\n",
    "def dig_missing_values(\n",
    "    file_path: str,\n",
    "    output_dir: str,\n",
    "    obs_rate: float = 0.6,\n",
    "    mar_missing_rate: float = 0.4,\n",
    "    mnar_offset: float = 0.7,\n",
    "    mcar_p: float = 0.1\n",
    "):\n",
    "    # ËØªÂèñÊï∞ÊçÆ\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = df.values.astype(np.float32)\n",
    "    \n",
    "    # Â∫îÁî®Áº∫Â§±Êú∫Âà∂\n",
    "    X = mar_logistic(data, obs_rate=obs_rate, missing_rate=mar_missing_rate)\n",
    "    X = X[np.newaxis, ...]                  # Ê∑ªÂä† batch Áª¥Â∫¶\n",
    "    X = mnar_x(X, offset=mnar_offset)       # MNAR Áº∫Â§±\n",
    "    X = mcar(X, p=mcar_p)                   # ÂÜçÊ¨°Ê∑ªÂä† MCAR Áº∫Â§±\n",
    "    X = X.squeeze(0)                        # ÂéªÈô§ batch Áª¥Â∫¶\n",
    "\n",
    "    # ‰øùÂ≠òÁªìÊûú\n",
    "    filename = os.path.basename(file_path).replace('.csv', '_dig_missing.csv')\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    pd.DataFrame(X).to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ ‰øùÂ≠òËá≥: {output_path}\")\n",
    "dig_missing_values(\n",
    "    file_path='./data/air/2013-03-07.csv',\n",
    "    output_dir='./',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsdb\n",
    "\n",
    "# list all available datasets in TSDB\n",
    "tsdb.list()\n",
    "# ['physionet_2012',\n",
    "#  'physionet_2019',\n",
    "#  'electricity_load_diagrams',\n",
    "#  'beijing_multisite_air_quality',\n",
    "#  'italy_air_quality',\n",
    "#  'vessel_ais',\n",
    "#  'electricity_transformer_temperature',\n",
    "#  'pems_traffic',\n",
    "#  'solar_alabama',\n",
    "#  'ucr_uea_ACSF1',\n",
    "#  'ucr_uea_Adiac',\n",
    "#  ...\n",
    "\n",
    "tsdb.download_and_extract('beijing_multisite_air_quality', './save_it_here')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==== Áî®Êà∑Ëá™ÂÆö‰πâË∑ØÂæÑ ====\n",
    "input_file = './save_it_here/PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv'       # ËæìÂÖ•Êñá‰ª∂Ë∑ØÂæÑ\n",
    "output_dir = './data/air'     # ËæìÂá∫Êñá‰ª∂Â§πË∑ØÂæÑ\n",
    "\n",
    "# ==== ÈúÄË¶ÅÊèêÂèñÁöÑÂàó ====\n",
    "columns_needed = [\"year\", \"month\", \"day\", \"hour\", \"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\",\n",
    "                  \"O3\", \"TEMP\", \"PRES\", \"DEWP\", \"RAIN\", \"WSPM\"]\n",
    "\n",
    "# ==== ÂàõÂª∫ËæìÂá∫ÁõÆÂΩïÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºâ ====\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ==== ËØªÂèñÊï∞ÊçÆÂπ∂ÊèêÂèñÊåáÂÆöÂàó ====\n",
    "df = pd.read_csv(input_file, usecols=columns_needed)\n",
    "\n",
    "# ==== ÊåâÊó•ÊúüÂàÜÁªÑÔºàyear, month, dayÔºâ ====\n",
    "grouped = df.groupby(['year', 'month', 'day'])\n",
    "\n",
    "# ==== ÈÅçÂéÜÊØè‰∏ÄÂ§©ÁöÑÊï∞ÊçÆ ====\n",
    "for (y, m, d), group in grouped:\n",
    "    # Ê£ÄÊü•ÊòØÂê¶‰∏∫Êï¥Â§©Ôºà24Â∞èÊó∂Ôºâ‰∏îÊó†Áº∫Â§±ÂÄº\n",
    "    if len(group) == 24 and not group.isnull().values.any():\n",
    "        # ÊûÑÈÄ†‰øùÂ≠òÊñá‰ª∂Âêç\n",
    "        filename = f\"{y:04d}-{m:02d}-{d:02d}.csv\"\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "        group.to_csv(save_path, index=False,)\n",
    "        print(f\"‚úÖ Â∑≤‰øùÂ≠ò: {filename}\")\n",
    "\n",
    "print(\"üéâ ÊâÄÊúâÂÆåÊï¥Â§©Êï∞ÊçÆÊèêÂèñÂÆåÊàêÔºÅ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e56633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_last_n_files(folder_path, n, sort_by='name'):\n",
    "    \"\"\"\n",
    "    Âà†Èô§ÊåáÂÆöÊñá‰ª∂Â§π‰∏ãÁöÑÊúÄÂêé n ‰∏™Êñá‰ª∂„ÄÇ\n",
    "    \n",
    "    :param folder_path: Êñá‰ª∂Â§πË∑ØÂæÑ\n",
    "    :param n: Ë¶ÅÂà†Èô§ÁöÑÊñá‰ª∂‰∏™Êï∞\n",
    "    :param sort_by: 'name' Êàñ 'mtime'ÔºåÊåâÊñá‰ª∂ÂêçÊàñÊúÄÂêé‰øÆÊîπÊó∂Èó¥ÊéíÂ∫è\n",
    "    \"\"\"\n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "             if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if sort_by == 'name':\n",
    "        files.sort()  # ÊåâÊñá‰ª∂ÂêçÊéíÂ∫è\n",
    "    elif sort_by == 'mtime':\n",
    "        files.sort(key=os.path.getmtime)  # Êåâ‰øÆÊîπÊó∂Èó¥ÊéíÂ∫è\n",
    "    else:\n",
    "        raise ValueError(\"sort_by must be 'name' or 'mtime'\")\n",
    "\n",
    "    to_delete = files[-n:]  # ÂèñÊúÄÂêé n ‰∏™Êñá‰ª∂\n",
    "\n",
    "    for file_path in to_delete:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"‚úÖ Â∑≤Âà†Èô§: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Âà†Èô§Â§±Ë¥•: {file_path}, ÈîôËØØ: {e}\")\n",
    "\n",
    "# Á§∫‰æãÁî®Ê≥ï\n",
    "delete_last_n_files(\"./data/air\", n=575, sort_by='name')  # Êàñ sort_by='mtime'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4dc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_matching_files(dir_a, dir_b, dir_c):\n",
    "    if not os.path.exists(dir_c):\n",
    "        os.makedirs(dir_c)\n",
    "\n",
    "    # Ëé∑ÂèñÁõÆÂΩï B ‰∏≠ÊâÄÊúâÊñá‰ª∂ÂêçÔºà‰∏çÂê´Ë∑ØÂæÑÔºâ\n",
    "    b_filenames = set(os.listdir(dir_b))\n",
    "\n",
    "    # ÈÅçÂéÜÁõÆÂΩï AÔºåÊü•Êâæ‰∏éÁõÆÂΩï B ÂêåÂêçÁöÑÊñá‰ª∂\n",
    "    for filename in os.listdir(dir_a):\n",
    "        if filename in b_filenames:\n",
    "            src_path = os.path.join(dir_a, filename)\n",
    "            dst_path = os.path.join(dir_c, filename)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            print(f\"‚úÖ Â∑≤Â§çÂà∂: {filename}\")\n",
    "\n",
    "# ‚úÖ Á§∫‰æã‰ΩøÁî®Ôºö\n",
    "dir_a = \"./data/III\"# ÊõøÊç¢‰∏∫ÂÆûÈôÖË∑ØÂæÑ\n",
    "dir_b = \"./data_imputed/my_model/III\"\n",
    "dir_c = \"./data/downstreamIII\"  # ÊõøÊç¢‰∏∫ÂÆûÈôÖË∑ØÂæÑ\n",
    "copy_matching_files(dir_a, dir_b, dir_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e70264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def pad_csv_files_to_193_rows(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue  # Ë∑≥ËøáÈùû CSV Êñá‰ª∂\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            original_rows = len(df)\n",
    "            target_rows = 192  # ‰∏çÂåÖÂê´Ë°®Â§¥ÁöÑÈÉ®ÂàÜ\n",
    "\n",
    "            if original_rows < target_rows:\n",
    "                # Ëé∑ÂèñÊúÄÂêé‰∏ÄË°åÔºàÂ¶ÇÊûú‰∏∫Á©∫ÂàôË∑≥ËøáÔºâ\n",
    "                if original_rows == 0:\n",
    "                    print(f\"‚ö†Ô∏è Êñá‰ª∂ {filename} ÊòØÁ©∫ÁöÑÔºåË∑≥Ëøá\")\n",
    "                    continue\n",
    "\n",
    "                last_row = df.iloc[[-1]]  # ‰øùÊåÅ DataFrame Ê†ºÂºè\n",
    "                rows_to_add = target_rows - original_rows\n",
    "\n",
    "                # ÈáçÂ§çËøΩÂä†ÊúÄÂêé‰∏ÄË°å\n",
    "                df = pd.concat([df] + [last_row] * rows_to_add, ignore_index=True)\n",
    "                df.to_csv(filepath, index=False)\n",
    "                print(f\"‚úÖ Êñá‰ª∂ {filename} Ë°•ÈΩêËá≥ {target_rows+1} Ë°åÔºàÂê´Ë°®Â§¥Ôºâ\")\n",
    "\n",
    "            elif original_rows > target_rows:\n",
    "                print(f\"‚úÖ Êñá‰ª∂ {filename} Â∑≤Êª°Ë∂≥Ë°åÊï∞Ë¶ÅÊ±Ç ({original_rows+1} Ë°åÂê´Ë°®Â§¥)\")\n",
    "\n",
    "            else:\n",
    "                print(f\"‚úÖ Êñá‰ª∂ {filename} ÊÅ∞Â•Ω {target_rows+1} Ë°åÔºàÂê´Ë°®Â§¥Ôºâ\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Â§ÑÁêÜÊñá‰ª∂ {filename} Âá∫Èîô: {e}\")\n",
    "\n",
    "# ‚úÖ Á§∫‰æãÁî®Ê≥ï\n",
    "target_dir = \"./data_imputed/my_model/III\"  # ÊõøÊç¢‰∏∫ÂÆûÈôÖÁõÆÂΩïË∑ØÂæÑ\n",
    "pad_csv_files_to_193_rows(target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb2576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICEÂ°´Ë°•‰∏≠:   2%|‚ñè         | 20/1022 [01:21<1:02:39,  3.75s/it]/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/linear_model/_bayes.py:355: RuntimeWarning: overflow encountered in multiply\n",
      "  gamma_ = np.sum((alpha_ * eigen_vals_) / (lambda_ + alpha_ * eigen_vals_))\n",
      "/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/linear_model/_bayes.py:355: RuntimeWarning: invalid value encountered in divide\n",
      "  gamma_ = np.sum((alpha_ * eigen_vals_) / (lambda_ + alpha_ * eigen_vals_))\n",
      "MICEÂ°´Ë°•‰∏≠:   2%|‚ñè         | 20/1022 [01:21<1:08:18,  4.09s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nBayesianRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, fname)\n\u001b[1;32m     42\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 43\u001b[0m filled \u001b[38;5;241m=\u001b[39m \u001b[43mmice_impu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(filled)\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, fname), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[50], line 31\u001b[0m, in \u001b[0;36mmice_impu\u001b[0;34m(mx, max_iter)\u001b[0m\n\u001b[1;32m     29\u001b[0m         model \u001b[38;5;241m=\u001b[39m BayesianRidge()\n\u001b[1;32m     30\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 31\u001b[0m         matrix_filled[missing_idx, col] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matrix_filled\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/linear_model/_bayes.py:415\u001b[0m, in \u001b[0;36mBayesianRidge.predict\u001b[0;34m(self, X, return_std)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, return_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    394\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    In addition to the mean of the predictive distribution, also its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m        Standard deviation of predictive distribution of query points.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m     y_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_std:\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m y_mean\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/linear_model/_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nBayesianRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# ==== Â°´Ë°•ÂáΩÊï∞ ====\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import numpy as np\n",
    "\n",
    "def mice_impu(mx, max_iter=5, n_jobs=-1):\n",
    "    mx = mx.copy()\n",
    "    n_rows, n_cols = mx.shape\n",
    "    all_nan_cols = np.all(np.isnan(mx), axis=0)\n",
    "    if all_nan_cols.any():\n",
    "        global_mean = np.nanmean(mx)\n",
    "        mx[:, all_nan_cols] = global_mean\n",
    "\n",
    "    imp = SimpleImputer(strategy='mean')\n",
    "    matrix_filled = imp.fit_transform(mx)\n",
    "\n",
    "    def update_column(col):\n",
    "        missing_idx = np.where(np.isnan(mx[:, col]))[0]\n",
    "        if len(missing_idx) == 0:\n",
    "            return None  # ‰∏çÊõ¥Êñ∞\n",
    "\n",
    "        observed_idx = np.where(~np.isnan(mx[:, col]))[0]\n",
    "        X_train = np.delete(matrix_filled[observed_idx], col, axis=1)\n",
    "        y_train = mx[observed_idx, col]\n",
    "        X_pred = np.delete(matrix_filled[missing_idx], col, axis=1)\n",
    "\n",
    "        model = BayesianRidge()\n",
    "        model.fit(X_train, y_train)\n",
    "        return col, missing_idx, model.predict(X_pred)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        results = Parallel(n_jobs=n_jobs)(delayed(update_column)(col) for col in range(n_cols))\n",
    "        for res in results:\n",
    "            if res is not None:\n",
    "                col, missing_idx, preds = res\n",
    "                matrix_filled[missing_idx, col] = preds\n",
    "\n",
    "    return matrix_filled\n",
    "\n",
    "# ==== ‰∏ªÈÄªËæë ====\n",
    "input_dir = './data/downstreamIII'\n",
    "output_dir = './data_imputed/mice/III'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for fname in tqdm(os.listdir(input_dir), desc=\"MICEÂ°´Ë°•‰∏≠\"):\n",
    "    if fname.endswith('.csv'):\n",
    "        path = os.path.join(input_dir, fname)\n",
    "        data = pd.read_csv(path).values.astype(np.float32)\n",
    "        filled = mice_impu(data)\n",
    "        pd.DataFrame(filled).to_csv(os.path.join(output_dir, fname), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
