{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06111fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_impute import *\n",
    "from pygrinder import block_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/lorenz/lorenz_dataset_0_timeseries.csv\",header=None)\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35727148",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = pd.read_csv('./causality_matrices/lorenz_causality_matrix.csv', header=None)\n",
    "cg = cg.values\n",
    "model_params = {\n",
    "        'num_levels': 10,\n",
    "        'kernel_size': 8,\n",
    "        'dilation_c': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = block_missing(data[np.newaxis,...],factor=0.1, block_len=3, block_width=3)\n",
    "data = data[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3939b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed = impute(data, cg, model_params)\n",
    "data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd675b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_miracle.py\n",
    "import numpy as np\n",
    "from baseline import miracle_impu\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•æ•°æ®\n",
    "test_data = np.random.randn(100, 10)\n",
    "test_data[np.random.random((100, 10)) < 0.1] = np.nan\n",
    "\n",
    "print(\"æµ‹è¯•æ•°æ®å½¢çŠ¶:\", test_data.shape)\n",
    "print(\"ç¼ºå¤±å€¼æ•°é‡:\", np.isnan(test_data).sum())\n",
    "\n",
    "try:\n",
    "    result = miracle_impu(test_data)\n",
    "    print(\"MIRACLEç»“æœå½¢çŠ¶:\", result.shape if result is not None else \"None\")\n",
    "    print(\"MIRACLEç»“æœç±»å‹:\", type(result))\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"å¡«è¡¥åç¼ºå¤±å€¼:\", np.isnan(result).sum())\n",
    "    else:\n",
    "        print(\"âŒ MIRACLEè¿”å›äº†None\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ MIRACLEæµ‹è¯•å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf579a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# è¯»å– CSV æ–‡ä»¶\n",
    "zero_matrix = pd.read_csv(\"zero_impu_matrix.csv\").values  # shape: (T, N)\n",
    "\n",
    "# æ„é€ æ©ç çŸ©é˜µï¼š0 çš„ä½ç½®ä¸º 0ï¼Œå…¶ä½™ä¸º 1\n",
    "mask_matrix = np.where(zero_matrix == 0, 0, 1)\n",
    "\n",
    "# ä¿å­˜æ–°çŸ©é˜µåˆ° CSV\n",
    "pd.DataFrame(mask_matrix).to_csv(\"zero_impu_mask.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa10119",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv('./train_result.csv').values\n",
    "gt = pd.read_csv('./gt_matrix.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbe677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = 1 - mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred*mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64647142",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt*mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = F.mse_loss(torch.tensor(pred*mask_matrix), torch.tensor(gt*mask_matrix))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f44c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "tmp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "tmp_file.close()\n",
    "os.remove(tmp_file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a02438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# åŸå§‹ç›®å½•å’Œç›®æ ‡ç›®å½•è·¯å¾„\n",
    "src_dir = './data/III'        # æ›¿æ¢ä¸ºä½ çš„æºç›®å½•è·¯å¾„\n",
    "dst_dir = './data/mimic-iii'   # æ›¿æ¢ä¸ºä½ çš„ç›®æ ‡ç›®å½•è·¯å¾„\n",
    "\n",
    "# åˆ›å»ºç›®æ ‡ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# è·å–æºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶åï¼ˆæŒ‰åç§°æ’åºï¼Œå¯æ”¹ä¸ºæŒ‰ä¿®æ”¹æ—¶é—´ç­‰ï¼‰\n",
    "all_files = sorted(f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f)))\n",
    "\n",
    "# é€‰æ‹©å‰100ä¸ªæ–‡ä»¶\n",
    "files_to_move = all_files[:100]\n",
    "\n",
    "# ç§»åŠ¨æ–‡ä»¶\n",
    "for fname in files_to_move:\n",
    "    shutil.move(os.path.join(src_dir, fname), os.path.join(dst_dir, fname))\n",
    "\n",
    "print(f\"å·²æˆåŠŸç§»åŠ¨ {len(files_to_move)} ä¸ªæ–‡ä»¶åˆ° {dst_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9813c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from baseline import *\n",
    "from models_impute import *\n",
    "# æŒ‡å®šç›®å½•\n",
    "data_dir = \"./data/III\"  # æ›¿æ¢ä¸ºä½ çš„ç›®å½•è·¯å¾„\n",
    "\n",
    "# è·å–ç›®å½•ä¸‹ç¬¬ä¸€ä¸ª CSV æ–‡ä»¶\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "assert csv_files, \"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ° CSV æ–‡ä»¶\"\n",
    "csv_path = os.path.join(data_dir, csv_files[0])\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_csv(csv_path)\n",
    "mx = df.values.astype(np.float32)\n",
    "\n",
    "# è®°å½•åŸå§‹ç¼ºå¤±ä½ç½®\n",
    "nan_pos = np.argwhere(np.isnan(mx))\n",
    "\n",
    "# å¡«è¡¥ç¼ºå¤±å€¼\n",
    "imputed_mx = timesnet_impu(mx,)\n",
    "\n",
    "# æ‰“å°å¡«è¡¥å‰åçš„å€¼ï¼ˆä»…ç¼ºå¤±ä½ç½®ï¼‰\n",
    "print(f\"å¡«è¡¥å‰åå¯¹æ¯”ï¼ˆä»…ç¼ºå¤±ä½ç½®ï¼‰ï¼š{imputed_mx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import sys\n",
    "\n",
    "for _, name, _ in pkgutil.iter_modules():\n",
    "    if 'tsde' in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygrinder import mcar, mar_logistic, mnar_x\n",
    "\n",
    "def dig_missing_values(\n",
    "    file_path: str,\n",
    "    output_dir: str,\n",
    "    obs_rate: float = 0.6,\n",
    "    mar_missing_rate: float = 0.4,\n",
    "    mnar_offset: float = 0.7,\n",
    "    mcar_p: float = 0.1\n",
    "):\n",
    "    # è¯»å–æ•°æ®\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = df.values.astype(np.float32)\n",
    "    \n",
    "    # åº”ç”¨ç¼ºå¤±æœºåˆ¶\n",
    "    X = mar_logistic(data, obs_rate=obs_rate, missing_rate=mar_missing_rate)\n",
    "    X = X[np.newaxis, ...]                  # æ·»åŠ  batch ç»´åº¦\n",
    "    X = mnar_x(X, offset=mnar_offset)       # MNAR ç¼ºå¤±\n",
    "    X = mcar(X, p=mcar_p)                   # å†æ¬¡æ·»åŠ  MCAR ç¼ºå¤±\n",
    "    X = X.squeeze(0)                        # å»é™¤ batch ç»´åº¦\n",
    "\n",
    "    # ä¿å­˜ç»“æœ\n",
    "    filename = os.path.basename(file_path).replace('.csv', '_dig_missing.csv')\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    pd.DataFrame(X).to_csv(output_path, index=False)\n",
    "    print(f\"âœ… ä¿å­˜è‡³: {output_path}\")\n",
    "dig_missing_values(\n",
    "    file_path='./data/air/2013-03-07.csv',\n",
    "    output_dir='./',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsdb\n",
    "\n",
    "# list all available datasets in TSDB\n",
    "tsdb.list()\n",
    "# ['physionet_2012',\n",
    "#  'physionet_2019',\n",
    "#  'electricity_load_diagrams',\n",
    "#  'beijing_multisite_air_quality',\n",
    "#  'italy_air_quality',\n",
    "#  'vessel_ais',\n",
    "#  'electricity_transformer_temperature',\n",
    "#  'pems_traffic',\n",
    "#  'solar_alabama',\n",
    "#  'ucr_uea_ACSF1',\n",
    "#  'ucr_uea_Adiac',\n",
    "#  ...\n",
    "\n",
    "tsdb.download_and_extract('beijing_multisite_air_quality', './save_it_here')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==== ç”¨æˆ·è‡ªå®šä¹‰è·¯å¾„ ====\n",
    "input_file = './save_it_here/PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv'       # è¾“å…¥æ–‡ä»¶è·¯å¾„\n",
    "output_dir = './data/air'     # è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„\n",
    "\n",
    "# ==== éœ€è¦æå–çš„åˆ— ====\n",
    "columns_needed = [\"year\", \"month\", \"day\", \"hour\", \"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\",\n",
    "                  \"O3\", \"TEMP\", \"PRES\", \"DEWP\", \"RAIN\", \"WSPM\"]\n",
    "\n",
    "# ==== åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ====\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ==== è¯»å–æ•°æ®å¹¶æå–æŒ‡å®šåˆ— ====\n",
    "df = pd.read_csv(input_file, usecols=columns_needed)\n",
    "\n",
    "# ==== æŒ‰æ—¥æœŸåˆ†ç»„ï¼ˆyear, month, dayï¼‰ ====\n",
    "grouped = df.groupby(['year', 'month', 'day'])\n",
    "\n",
    "# ==== éå†æ¯ä¸€å¤©çš„æ•°æ® ====\n",
    "for (y, m, d), group in grouped:\n",
    "    # æ£€æŸ¥æ˜¯å¦ä¸ºæ•´å¤©ï¼ˆ24å°æ—¶ï¼‰ä¸”æ— ç¼ºå¤±å€¼\n",
    "    if len(group) == 24 and not group.isnull().values.any():\n",
    "        # æ„é€ ä¿å­˜æ–‡ä»¶å\n",
    "        filename = f\"{y:04d}-{m:02d}-{d:02d}.csv\"\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "        group.to_csv(save_path, index=False,)\n",
    "        print(f\"âœ… å·²ä¿å­˜: {filename}\")\n",
    "\n",
    "print(\"ğŸ‰ æ‰€æœ‰å®Œæ•´å¤©æ•°æ®æå–å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e56633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_last_n_files(folder_path, n, sort_by='name'):\n",
    "    \"\"\"\n",
    "    åˆ é™¤æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æœ€å n ä¸ªæ–‡ä»¶ã€‚\n",
    "    \n",
    "    :param folder_path: æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    :param n: è¦åˆ é™¤çš„æ–‡ä»¶ä¸ªæ•°\n",
    "    :param sort_by: 'name' æˆ– 'mtime'ï¼ŒæŒ‰æ–‡ä»¶åæˆ–æœ€åä¿®æ”¹æ—¶é—´æ’åº\n",
    "    \"\"\"\n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "             if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if sort_by == 'name':\n",
    "        files.sort()  # æŒ‰æ–‡ä»¶åæ’åº\n",
    "    elif sort_by == 'mtime':\n",
    "        files.sort(key=os.path.getmtime)  # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº\n",
    "    else:\n",
    "        raise ValueError(\"sort_by must be 'name' or 'mtime'\")\n",
    "\n",
    "    to_delete = files[-n:]  # å–æœ€å n ä¸ªæ–‡ä»¶\n",
    "\n",
    "    for file_path in to_delete:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"âœ… å·²åˆ é™¤: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åˆ é™¤å¤±è´¥: {file_path}, é”™è¯¯: {e}\")\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "delete_last_n_files(\"./data/air\", n=575, sort_by='name')  # æˆ– sort_by='mtime'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4dc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_matching_files(dir_a, dir_b, dir_c):\n",
    "    if not os.path.exists(dir_c):\n",
    "        os.makedirs(dir_c)\n",
    "\n",
    "    # è·å–ç›®å½• B ä¸­æ‰€æœ‰æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰\n",
    "    b_filenames = set(os.listdir(dir_b))\n",
    "\n",
    "    # éå†ç›®å½• Aï¼ŒæŸ¥æ‰¾ä¸ç›®å½• B åŒåçš„æ–‡ä»¶\n",
    "    for filename in os.listdir(dir_a):\n",
    "        if filename in b_filenames:\n",
    "            src_path = os.path.join(dir_a, filename)\n",
    "            dst_path = os.path.join(dir_c, filename)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            print(f\"âœ… å·²å¤åˆ¶: {filename}\")\n",
    "\n",
    "# âœ… ç¤ºä¾‹ä½¿ç”¨ï¼š\n",
    "dir_a = \"./data/III\"# æ›¿æ¢ä¸ºå®é™…è·¯å¾„\n",
    "dir_b = \"./data_imputed/my_model/III\"\n",
    "dir_c = \"./data/downstreamIII\"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„\n",
    "copy_matching_files(dir_a, dir_b, dir_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e70264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def pad_csv_files_to_193_rows(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue  # è·³è¿‡é CSV æ–‡ä»¶\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            original_rows = len(df)\n",
    "            target_rows = 192  # ä¸åŒ…å«è¡¨å¤´çš„éƒ¨åˆ†\n",
    "\n",
    "            if original_rows < target_rows:\n",
    "                # è·å–æœ€åä¸€è¡Œï¼ˆå¦‚æœä¸ºç©ºåˆ™è·³è¿‡ï¼‰\n",
    "                if original_rows == 0:\n",
    "                    print(f\"âš ï¸ æ–‡ä»¶ {filename} æ˜¯ç©ºçš„ï¼Œè·³è¿‡\")\n",
    "                    continue\n",
    "\n",
    "                last_row = df.iloc[[-1]]  # ä¿æŒ DataFrame æ ¼å¼\n",
    "                rows_to_add = target_rows - original_rows\n",
    "\n",
    "                # é‡å¤è¿½åŠ æœ€åä¸€è¡Œ\n",
    "                df = pd.concat([df] + [last_row] * rows_to_add, ignore_index=True)\n",
    "                df.to_csv(filepath, index=False)\n",
    "                print(f\"âœ… æ–‡ä»¶ {filename} è¡¥é½è‡³ {target_rows+1} è¡Œï¼ˆå«è¡¨å¤´ï¼‰\")\n",
    "\n",
    "            elif original_rows > target_rows:\n",
    "                print(f\"âœ… æ–‡ä»¶ {filename} å·²æ»¡è¶³è¡Œæ•°è¦æ±‚ ({original_rows+1} è¡Œå«è¡¨å¤´)\")\n",
    "\n",
    "            else:\n",
    "                print(f\"âœ… æ–‡ä»¶ {filename} æ°å¥½ {target_rows+1} è¡Œï¼ˆå«è¡¨å¤´ï¼‰\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤„ç†æ–‡ä»¶ {filename} å‡ºé”™: {e}\")\n",
    "\n",
    "# âœ… ç¤ºä¾‹ç”¨æ³•\n",
    "target_dir = \"./data_imputed/my_model/III\"  # æ›¿æ¢ä¸ºå®é™…ç›®å½•è·¯å¾„\n",
    "pad_csv_files_to_193_rows(target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb2576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨ 4 ä¸ª GPU å¹¶è¡Œå¡«è¡¥ï¼Œå…± 1367 ä¸ªæ–‡ä»¶\n",
      "âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# saits_parallel.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from multiprocessing import Process, Queue, current_process\n",
    "from tqdm import tqdm\n",
    "\n",
    "def saits_impu(mx, epochs=50, d_model=32, n_layers=2, n_heads=4,\n",
    "               d_k=8, d_v=8, d_ffn=16, dropout=0.4, device=None):\n",
    "    from pypots.imputation import SAITS\n",
    "\n",
    "    global_mean = np.nanmean(mx)\n",
    "    all_nan_cols = np.all(np.isnan(mx), axis=0)\n",
    "    if all_nan_cols.any():\n",
    "        print(f\"å‘ç° {all_nan_cols.sum()} åˆ—å…¨ä¸ºNaNï¼Œè¿™äº›åˆ—å°†ç”¨å¡«å……\")\n",
    "        mx[:, all_nan_cols] = global_mean\n",
    "\n",
    "    mx = mx.copy()\n",
    "    n_steps, n_features = mx.shape\n",
    "    data_3d = mx[np.newaxis, :, :]\n",
    "\n",
    "    saits = SAITS(\n",
    "        n_steps=n_steps,\n",
    "        n_features=n_features,\n",
    "        n_layers=n_layers,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        d_k=d_k,\n",
    "        d_v=d_v,\n",
    "        d_ffn=d_ffn,\n",
    "        dropout=dropout,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    train_set = {\"X\": data_3d}\n",
    "    saits.fit(train_set)\n",
    "    imputed = saits.impute({\"X\": data_3d})\n",
    "    return imputed[0]\n",
    "\n",
    "def gpu_worker(file_queue, input_dir, output_dir, gpu_id):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pid = current_process().pid\n",
    "\n",
    "    while not file_queue.empty():\n",
    "        try:\n",
    "            fname = file_queue.get_nowait()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            fpath = os.path.join(input_dir, fname)\n",
    "            mx = pd.read_csv(fpath).values.astype(np.float32)\n",
    "            filled = saits_impu(mx, device=device)\n",
    "\n",
    "            out_path = os.path.join(output_dir, fname)\n",
    "            pd.DataFrame(filled).to_csv(out_path, index=False)\n",
    "            print(f\"[PID {pid} GPU {gpu_id}] âœ… å¤„ç†å®Œæˆ: {fname}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[PID {pid} GPU {gpu_id}] âŒ å¤„ç†å¤±è´¥: {fname}ï¼Œé”™è¯¯ï¼š{e}\")\n",
    "\n",
    "def parallel_saits_impute(input_dir, output_dir):\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_list = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    file_queue = mp.Queue()\n",
    "    for fname in file_list:\n",
    "        file_queue.put(fname)\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    processes = []\n",
    "\n",
    "    print(f\"ğŸš€ å¯åŠ¨ {num_gpus} ä¸ª GPU å¹¶è¡Œå¡«è¡¥ï¼Œå…± {len(file_list)} ä¸ªæ–‡ä»¶\")\n",
    "\n",
    "    for gpu_id in range(num_gpus):\n",
    "        p = mp.Process(target=gpu_worker, args=(file_queue, input_dir, output_dir, gpu_id))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    print(\"âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import multiprocessing as mp\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    input_dir = \"./data/downstreamIII\"\n",
    "    output_dir = \"./data_imputed/saits/III\"\n",
    "    parallel_saits_impute(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf869f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "tar = pd.read_csv('./data/mimic/200068.csv').values\n",
    "print(tar)\n",
    "from baseline import *\n",
    "res = knn_impu(tar, k=5)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
