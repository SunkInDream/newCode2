{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2eadf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative paths: ['./test_res/representative_group_1.csv', './test_res/representative_group_2.csv', './test_res/representative_group_3.csv', './test_res/representative_group_4.csv', './test_res/representative_group_5.csv', './test_res/representative_group_6.csv', './test_res/representative_group_7.csv', './test_res/representative_group_8.csv', './test_res/representative_group_9.csv', './test_res/representative_group_10.csv', './test_res/representative_group_11.csv', './test_res/representative_group_12.csv', './test_res/representative_group_13.csv', './test_res/representative_group_14.csv', './test_res/representative_group_15.csv', './test_res/representative_group_16.csv', './test_res/representative_group_17.csv', './test_res/representative_group_18.csv', './test_res/representative_group_19.csv', './test_res/representative_group_20.csv', './test_res/representative_group_21.csv', './test_res/representative_group_22.csv', './test_res/representative_group_23.csv', './test_res/representative_group_24.csv', './test_res/representative_group_25.csv', './test_res/representative_group_26.csv', './test_res/representative_group_27.csv', './test_res/representative_group_28.csv', './test_res/representative_group_29.csv', './test_res/representative_group_30.csv']\n",
      "Assignments saved to: ./test_res/assignments.csv\n",
      "Output directory: ./test_res\n",
      "Number of EM iterations: 2\n"
     ]
    }
   ],
   "source": [
    "# Modified version: pick a REAL matrix (medoid) per group as representative\n",
    "# - After EM, for each group g, we compute the standardized group mean curve M_g (T,F).\n",
    "# - Among samples whose hard assignment == g, choose the sample i minimizing ||Yz[i] - M_g||_F^2.\n",
    "# - If a group is empty (rare), fall back to global nearest by soft distance.\n",
    "#\n",
    "# Usage is the same:\n",
    "#   res = run_multitrajectory_clustering(input_dir=\"...\", n_groups=..., degree=2, output_dir=\"...\")\n",
    "#\n",
    "# Outputs now save, for each group:\n",
    "#   representative_group_{g}.csv  -> the chosen REAL sample matrix (not the model mean)\n",
    "#   representatives_map.csv       -> which original file was chosen and its distance\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def _read_csv_matrix(path: str) -> np.ndarray:\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.copy()\n",
    "    df = df.ffill().bfill()\n",
    "    if df.isna().any().any():\n",
    "        df = df.fillna(df.mean(numeric_only=True))\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "    return df.to_numpy(dtype=float)\n",
    "\n",
    "def _build_time_design(T: int, degree: int) -> np.ndarray:\n",
    "    t = np.linspace(0.0, 1.0, T)\n",
    "    X = np.vstack([t**d for d in range(degree+1)]).T\n",
    "    return X\n",
    "\n",
    "def _init_by_kmeans(vecs: np.ndarray, k: int, max_iter: int = 50, seed: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N, D = vecs.shape\n",
    "    centroids = np.empty((k, D))\n",
    "    centroids[0] = vecs[rng.randint(N)]\n",
    "    for c in range(1, k):\n",
    "        dists = np.min(((vecs[:, None, :] - centroids[None, :c, :])**2).sum(axis=2), axis=1)\n",
    "        probs = dists / (dists.sum() + 1e-12)\n",
    "        idx = rng.choice(N, p=probs)\n",
    "        centroids[c] = vecs[idx]\n",
    "    labels = np.zeros(N, dtype=int)\n",
    "    for _ in range(max_iter):\n",
    "        d2 = ((vecs[:, None, :] - centroids[None, :, :])**2).sum(axis=2)\n",
    "        new_labels = np.argmin(d2, axis=1)\n",
    "        if np.all(new_labels == labels):\n",
    "            break\n",
    "        labels = new_labels\n",
    "        for c in range(k):\n",
    "            mask = labels == c\n",
    "            if mask.any():\n",
    "                centroids[c] = vecs[mask].mean(axis=0)\n",
    "            else:\n",
    "                centroids[c] = vecs[rng.randint(N)]\n",
    "    return centroids, labels\n",
    "\n",
    "def _weighted_least_squares(X: np.ndarray, Y: np.ndarray, w: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    w = np.asarray(w, dtype=float) + 1e-12\n",
    "    WX = X * w[:, None]\n",
    "    XtWX = X.T @ WX\n",
    "    XtWy = X.T @ (w * Y)\n",
    "    reg = 1e-6 * np.eye(XtWX.shape[0])\n",
    "    beta = np.linalg.solve(XtWX + reg, XtWy)\n",
    "    resid = Y - X @ beta\n",
    "    eff_dof = max(w.sum() - X.shape[1], 1.0)\n",
    "    sigma2 = float((w * resid**2).sum() / eff_dof)\n",
    "    sigma2 = max(sigma2, 1e-8)\n",
    "    return beta, sigma2\n",
    "\n",
    "def run_multitrajectory_clustering(\n",
    "    input_dir: str,\n",
    "    n_groups: int = 3,\n",
    "    degree: int = 2,\n",
    "    max_em_iter: int = 100,\n",
    "    tol: float = 1e-4,\n",
    "    seed: int = 42,\n",
    "    output_dir: str = None\n",
    ") -> Dict[str, any]:\n",
    "    assert os.path.isdir(input_dir), f\"Directory not found: {input_dir}\"\n",
    "    paths = sorted(glob.glob(os.path.join(input_dir, \"*.csv\")))\n",
    "    assert paths, f\"No CSV files found in: {input_dir}\"\n",
    "    \n",
    "    # Load\n",
    "    matrices = [ _read_csv_matrix(p) for p in paths ]\n",
    "    shapes = {m.shape for m in matrices}\n",
    "    assert len(shapes) == 1, f\"All CSVs must have the same shape, found shapes: {shapes}\"\n",
    "    T, F = matrices[0].shape\n",
    "    N = len(matrices)\n",
    "    Y = np.stack(matrices, axis=0)  # (N, T, F)\n",
    "    \n",
    "    # Standardize per-feature across (N,T)\n",
    "    Y_flat = Y.reshape(N*T, F)\n",
    "    meanF = Y_flat.mean(axis=0, keepdims=True)\n",
    "    stdF = Y_flat.std(axis=0, keepdims=True) + 1e-12\n",
    "    Yz = (Y_flat - meanF) / stdF\n",
    "    Yz = Yz.reshape(N, T, F)\n",
    "    \n",
    "    X = _build_time_design(T, degree)  # (T,P)\n",
    "    P = X.shape[1]\n",
    "    \n",
    "    # Init responsibilities via k-means\n",
    "    vecs = Yz.reshape(N, T*F)\n",
    "    _, labels = _init_by_kmeans(vecs, n_groups, seed=seed)\n",
    "    R = np.zeros((N, n_groups))\n",
    "    R[np.arange(N), labels] = 1.0\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    pi = R.mean(axis=0)\n",
    "    beta = rng.normal(scale=0.01, size=(n_groups, F, P))\n",
    "    sigma2 = np.ones((n_groups, F))\n",
    "    \n",
    "    def sample_group_loglik(y_if: np.ndarray) -> np.ndarray:\n",
    "        ll = np.zeros(n_groups)\n",
    "        for g in range(n_groups):\n",
    "            means = X @ beta[g].T  # (T,F)\n",
    "            resid2 = (y_if - means)**2\n",
    "            s2 = sigma2[g][None, :]\n",
    "            ll_f = -0.5 * (T * np.log(2*np.pi*s2) + (resid2 / s2).sum(axis=0))\n",
    "            ll[g] = ll_f.sum()\n",
    "        return ll\n",
    "    \n",
    "    prev_elbo = -np.inf\n",
    "    for it in range(max_em_iter):\n",
    "        # M-step\n",
    "        pi = R.mean(axis=0) + 1e-12\n",
    "        pi = pi / pi.sum()\n",
    "        X_stack = np.tile(X, (N, 1))  # (N*T, P)\n",
    "        for g in range(n_groups):\n",
    "            w_i = R[:, g]\n",
    "            w_stack = np.repeat(w_i, T)\n",
    "            for f in range(F):\n",
    "                y_stack = Yz[:, :, f].reshape(N*T)\n",
    "                b, s2 = _weighted_least_squares(X_stack, y_stack, w_stack)\n",
    "                beta[g, f, :] = b\n",
    "                sigma2[g, f] = s2\n",
    "        \n",
    "        # E-step\n",
    "        log_pi = np.log(pi + 1e-12)\n",
    "        logR = np.zeros((N, n_groups))\n",
    "        for i in range(N):\n",
    "            ll = sample_group_loglik(Yz[i])\n",
    "            logR[i] = ll + log_pi\n",
    "        logR = logR - logR.max(axis=1, keepdims=True)\n",
    "        R_new = np.exp(logR)\n",
    "        R_new = R_new / R_new.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # ELBO proxy\n",
    "        elbo = 0.0\n",
    "        for i in range(N):\n",
    "            ll = sample_group_loglik(Yz[i]) + log_pi\n",
    "            m = ll.max()\n",
    "            elbo += m + math.log(np.exp(ll - m).sum())\n",
    "        \n",
    "        R = R_new\n",
    "        if it > 0 and abs(elbo - prev_elbo) < tol * (1 + abs(prev_elbo)):\n",
    "            break\n",
    "        prev_elbo = elbo\n",
    "    \n",
    "    # Hard assignments\n",
    "    hard = R.argmax(axis=1)\n",
    "    \n",
    "    # Build group mean curves (standardized) for distance computation\n",
    "    group_means_z = [(X @ beta[g].T) for g in range(n_groups)]  # list of (T,F)\n",
    "    \n",
    "    # Choose medoid per group: nearest REAL sample in standardized space\n",
    "    rep_indices = []\n",
    "    rep_distances = []\n",
    "    for g in range(n_groups):\n",
    "        members = np.where(hard == g)[0]\n",
    "        if members.size == 0:\n",
    "            # fallback to all with soft weighting; pick global nearest\n",
    "            members = np.arange(N)\n",
    "        Mz = group_means_z[g]  # (T,F)\n",
    "        # Frobenius norm squared distances\n",
    "        dists = np.array([np.sum((Yz[i] - Mz)**2) for i in members])\n",
    "        idx_local = members[np.argmin(dists)]\n",
    "        rep_indices.append(int(idx_local))\n",
    "        rep_distances.append(float(dists[np.argmin(dists)]))\n",
    "    \n",
    "    # Save outputs\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.join(\"/mnt/data\", f\"gbtm_multitraj_results_k{n_groups}_deg{degree}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    rep_paths = []\n",
    "    for g, i_idx in enumerate(rep_indices, start=1):\n",
    "        df_rep = pd.DataFrame(Y[i_idx], columns=[f\"feat_{j}\" for j in range(F)])\n",
    "        out_path = os.path.join(output_dir, f\"representative_group_{g}.csv\")\n",
    "        df_rep.to_csv(out_path, index=False)\n",
    "        rep_paths.append(out_path)\n",
    "    \n",
    "    # Save assignment + which sample chosen per group\n",
    "    assign_df = pd.DataFrame({\n",
    "        \"file\": paths,\n",
    "        \"group\": (hard + 1),\n",
    "        **{f\"resp_{g+1}\": R[:, g] for g in range(n_groups)}\n",
    "    })\n",
    "    assign_path = os.path.join(output_dir, \"assignments.csv\")\n",
    "    assign_df.to_csv(assign_path, index=False)\n",
    "    \n",
    "    rep_map = pd.DataFrame({\n",
    "        \"group\": list(range(1, n_groups+1)),\n",
    "        \"chosen_index\": rep_indices,\n",
    "        \"chosen_file\": [paths[i] for i in rep_indices],\n",
    "        \"distance_to_mean_curve_z2\": rep_distances\n",
    "    })\n",
    "    rep_map_path = os.path.join(output_dir, \"representatives_map.csv\")\n",
    "    rep_map.to_csv(rep_map_path, index=False)\n",
    "    \n",
    "    return {\n",
    "        \"representative_paths\": rep_paths,\n",
    "        \"representatives_map_path\": rep_map_path,\n",
    "        \"assignments_path\": assign_path,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"n_iter\": it + 1\n",
    "    }\n",
    "\n",
    "res = run_multitrajectory_clustering(\n",
    "    input_dir=\"./data/lorenz\",\n",
    "    n_groups=3,\n",
    "    degree=2,\n",
    "    output_dir=\"./test_res\"\n",
    ")\n",
    "print(\"Representatives:\", res[\"representative_paths\"])\n",
    "print(\"Assignments:\", res[\"assignments_path\"])\n",
    "print(\"Map:\", res[\"representatives_map_path\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
